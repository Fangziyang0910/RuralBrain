"""
é€šç”¨æ–‡æ¡£åŠ è½½å™¨
æ”¯æŒ Markdownã€TXTã€PPTXã€PDFã€DOCXã€DOC ç­‰å¤šç§æ ¼å¼
ç¬¦åˆ LangChain æ–‡æ¡£åŠ è½½å™¨æ¥å£è§„èŒƒ
æ”¯æŒæŒ‰ç±»åˆ«ï¼ˆpolicies/casesï¼‰ç»„ç»‡çŸ¥è¯†åº“

æ‰€æœ‰æ ¼å¼éƒ½ä¼šç»Ÿä¸€è½¬æ¢ä¸º Markdownï¼Œå¹¶è‡ªåŠ¨æ¸…ç†å†—ä½™ä¿¡æ¯
"""
import os
import re
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Literal

from langchain_core.documents import Document
from pptx import Presentation
from pypdf import PdfReader
from docx import Document as DocxDocument
import filetype


# ==================== æ–‡ä»¶ç±»å‹æ£€æµ‹å·¥å…· ====================

class FileTypeDetector:
    """
    æ–‡ä»¶ç±»å‹æ£€æµ‹å·¥å…·
    ä¸ä¾èµ–æ‰©å±•åï¼Œé€šè¿‡æ–‡ä»¶å†…å®¹æ£€æµ‹çœŸå®ç±»å‹
    """

    # MIME ç±»å‹åˆ°æ–‡æ¡£ç±»å‹çš„æ˜ å°„
    MIME_TYPE_MAP = {
        # PDF
        'application/pdf': 'pdf',

        # Word æ–‡æ¡£
        'application/msword': 'doc',  # .doc
        'application/vnd.openxmlformats-officedocument.wordprocessingml.document': 'docx',
        'application/vnd.ms-word': 'doc',
        'application/wps-office.doc': 'doc',  # WPS .doc

        # PowerPoint
        'application/vnd.ms-powerpoint': 'ppt',  # .ppt
        'application/vnd.openxmlformats-officedocument.presentationml.presentation': 'pptx',

        # Excelï¼ˆç”¨äºé”™è¯¯æ£€æµ‹ï¼‰
        'application/vnd.ms-excel': 'xls',  # .xls
        'application/wps-office.xls': 'xls',  # WPS .xls

        # æ–‡æœ¬
        'text/plain': 'txt',
        'text/markdown': 'md',

        # OLE å¤åˆæ–‡æ¡£ï¼ˆè€æ ¼å¼ï¼‰
        'application/x-ole-storage': 'ole',  # å¯èƒ½æ˜¯ .doc, .ppt, .xls ç­‰
    }

    @staticmethod
    def detect(file_path: Path) -> str:
        """
        æ£€æµ‹æ–‡ä»¶çœŸå®ç±»å‹

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            æ–‡ä»¶ç±»å‹ï¼ˆpdf/doc/docx/ppt/pptx/txt/mdï¼‰
        """
        ext = file_path.suffix.lower()

        # å…ˆç”¨ filetype æ£€æµ‹
        kind = filetype.guess(str(file_path))

        if kind is None:
            # æ— æ³•æ£€æµ‹ï¼Œå°è¯•ä»æ‰©å±•åæ¨æ–­
            return FileTypeDetector._ext_to_type(ext)

        # æ£€æŸ¥ MIME ç±»å‹
        mime_type = kind.mime
        doc_type = FileTypeDetector.MIME_TYPE_MAP.get(mime_type)

        if doc_type:
            # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ‰©å±•åæ˜¯ .docx ä½†æ£€æµ‹åˆ°çš„æ˜¯ OLE æˆ– Excel ç±»å‹ï¼Œ
            # è¯´æ˜æ˜¯ä¼ªè£…æˆ .docx çš„ .doc æ–‡ä»¶
            if ext == '.docx' and mime_type in [
                'application/vnd.ms-excel',
                'application/wps-office.xls',
                'application/x-ole-storage'
            ]:
                print(f"âš ï¸  æ£€æµ‹åˆ°ä¼ªè£…æˆ .docx çš„ .doc æ–‡ä»¶: {file_path.name}")
                return 'doc'

            return doc_type

        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ£€æµ‹åˆ° OLE å¤åˆæ–‡æ¡£ï¼Œæ ¹æ®æ‰©å±•ååˆ¤æ–­
        if mime_type == 'application/x-ole-storage':
            if ext in ['.doc', '.ppt', '.xls']:
                return ext[1:]  # å»æ‰ç‚¹å·

        # æ— æ³•è¯†åˆ«ï¼Œå›é€€åˆ°æ‰©å±•å
        return FileTypeDetector._ext_to_type(ext)

    @staticmethod
    def _ext_to_type(ext: str) -> str:
        """æ‰©å±•ååˆ°ç±»å‹æ˜ å°„"""
        ext_map = {
            '.pdf': 'pdf',
            '.doc': 'doc',
            '.docx': 'docx',
            '.ppt': 'ppt',
            '.pptx': 'pptx',
            '.txt': 'txt',
            '.md': 'markdown',
        }
        return ext_map.get(ext.lower(), 'unknown')


# ==================== Markdown æ¸…ç†å·¥å…· ====================

class MarkdownCleaner:
    """
    Markdown å†…å®¹æ¸…ç†å·¥å…·
    å»é™¤æ ¼å¼æ•°æ®ã€å†—ä½™ä¿¡æ¯ã€ç©ºç™½è¿‡å¤šç­‰åƒåœ¾å†…å®¹
    """

    # éœ€è¦è¿‡æ»¤çš„é¡µçœ‰é¡µè„šæ¨¡å¼
    FOOTER_PATTERNS = [
        r'ç¬¬\s*\d+\s*é¡µ',
        r'Page\s*\d+',
        r'ä¿å¯†|æœºå¯†|å†…éƒ¨èµ„æ–™',
        r'www\.\w+\.com',
        r'http[s]?://\S+',
    ]

    # éœ€è¦è¿‡æ»¤çš„æ¨¡æ¿å ä½ç¬¦
    PLACEHOLDER_PATTERNS = [
        r'ç‚¹å‡»æ­¤å¤„æ·»åŠ .*',
        r'è¯·è¾“å…¥.*',
        r'\[.*?\]',  # æ–¹æ‹¬å·ä¸­çš„å ä½ç¬¦
        r'{{.*?}}',  # åŒèŠ±æ‹¬å·ä¸­çš„å ä½ç¬¦
    ]

    @staticmethod
    def clean_text(text: str) -> str:
        """
        æ¸…ç†æ–‡æœ¬å†…å®¹

        Args:
            text: åŸå§‹æ–‡æœ¬

        Returns:
            æ¸…ç†åçš„æ–‡æœ¬
        """
        # 1. å»é™¤é¡µçœ‰é¡µè„š
        for pattern in MarkdownCleaner.FOOTER_PATTERNS:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)

        # 2. å»é™¤æ¨¡æ¿å ä½ç¬¦
        for pattern in MarkdownCleaner.PLACEHOLDER_PATTERNS:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)

        # 3. å»é™¤è¿‡å¤šçš„ç©ºç™½è¡Œï¼ˆä¿ç•™æœ€å¤š 2 ä¸ªè¿ç»­ç©ºè¡Œï¼‰
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n\n', text)

        # 4. å»é™¤è¡Œé¦–è¡Œå°¾ç©ºç™½
        lines = [line.strip() for line in text.split('\n')]
        text = '\n'.join(lines)

        # 5. å»é™¤ç‰¹æ®Šå­—ç¬¦è¿‡å¤šä½†å†…å®¹å¾ˆå°‘çš„è¡Œ
        lines = text.split('\n')
        cleaned_lines = []
        for line in lines:
            # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦å’Œå­—æ¯æ•°å­—
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', line))
            alnum_chars = len(re.findall(r'[a-zA-Z0-9]', line))
            total_chars = len(line)

            # å¦‚æœæœ‰æ•ˆå­—ç¬¦å æ¯”è¶…è¿‡ 10% æˆ–ç»å¯¹æ•°é‡è¶…è¿‡ 5ï¼Œåˆ™ä¿ç•™
            if (chinese_chars + alnum_chars) / max(total_chars, 1) > 0.1 or (chinese_chars + alnum_chars) >= 5:
                cleaned_lines.append(line)

        text = '\n'.join(cleaned_lines)

        # 6. å»é™¤é¦–å°¾ç©ºç™½
        text = text.strip()

        return text

    @staticmethod
    def is_meaningful_content(text: str, min_length: int = 20) -> bool:
        """
        åˆ¤æ–­æ–‡æœ¬æ˜¯å¦æœ‰æ„ä¹‰

        Args:
            text: æ–‡æœ¬å†…å®¹
            min_length: æœ€å°é•¿åº¦é˜ˆå€¼

        Returns:
            æ˜¯å¦æœ‰æ„ä¹‰
        """
        # è¿‡æ»¤å¤ªçŸ­çš„æ–‡æœ¬
        if len(text.strip()) < min_length:
            return False

        # è¿‡æ»¤åªæœ‰ç¬¦å·çš„æ–‡æœ¬
        if not re.search(r'[\u4e00-\u9fff\u4e00-\u9fa5a-zA-Z0-9]', text):
            return False

        # è¿‡æ»¤çº¯æ•°å­—æˆ–æ—¥æœŸ
        if text.strip().replace('-', '').replace('/', '').replace(':', '').strip().isdigit():
            return False

        return True


# ==================== DOC åŠ è½½å™¨ï¼ˆLegacy Word æ ¼å¼ï¼‰====================

class DOCLoader:
    """
    Word æ–‡æ¡£åŠ è½½å™¨ï¼ˆDOC - Legacy Office 97-2003 æ ¼å¼ï¼‰
    ä½¿ç”¨ antiword æˆ– catdoc å‘½ä»¤è¡Œå·¥å…·æå–æ–‡æœ¬
    """

    def __init__(
        self,
        file_path: str | Path,
        category: Optional[Literal["policies", "cases"]] = None,
    ):
        self.file_path = Path(file_path)
        self.category = category
        self.cleaner = MarkdownCleaner()

    def _extract_with_antiword(self) -> str:
        """ä½¿ç”¨ antiword æå–æ–‡æœ¬"""
        try:
            result = subprocess.run(
                ['antiword', str(self.file_path)],
                capture_output=True,
                text=True,
                timeout=30
            )
            if result.returncode == 0:
                return result.stdout
            raise Exception(f"antiword failed with code {result.returncode}")
        except FileNotFoundError:
            raise Exception("antiword æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: sudo apt-get install antiword")
        except subprocess.TimeoutExpired:
            raise Exception("æ–‡æ¡£æå–è¶…æ—¶")

    def _extract_with_catdoc(self) -> str:
        """ä½¿ç”¨ catdoc æå–æ–‡æœ¬"""
        try:
            result = subprocess.run(
                ['catdoc', str(self.file_path)],
                capture_output=True,
                text=True,
                timeout=30
            )
            if result.returncode == 0:
                return result.stdout
            raise Exception(f"catdoc failed with code {result.returncode}")
        except FileNotFoundError:
            raise Exception("catdoc æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: sudo apt-get install catdoc")
        except subprocess.TimeoutExpired:
            raise Exception("æ–‡æ¡£æå–è¶…æ—¶")

    def load(self) -> List[Document]:
        """åŠ è½½ DOC æ–‡ä»¶å¹¶è¿”å›æ–‡æ¡£åˆ—è¡¨"""
        if not self.file_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {self.file_path}")

        print(f"ğŸ“ æ­£åœ¨è¯»å– Word æ–‡æ¡£ï¼ˆDOC æ ¼å¼ï¼‰: {self.file_path} ...")

        try:
            # å°è¯•ä½¿ç”¨ antiwordï¼ˆæ¨èï¼‰
            try:
                text = self._extract_with_antiword()
            except Exception as e:
                print(f"âš ï¸  antiword å¤±è´¥ï¼Œå°è¯• catdoc: {e}")
                text = self._extract_with_catdoc()

            # æ¸…ç†æ–‡æœ¬
            cleaned_text = self.cleaner.clean_text(text)

            # æŒ‰æ®µè½åˆ†å‰²
            paragraphs = [p.strip() for p in cleaned_text.split('\n\n') if p.strip()]

            documents = []
            for idx, paragraph in enumerate(paragraphs, start=1):
                if self.cleaner.is_meaningful_content(paragraph):
                    # è½¬æ¢ä¸º Markdown æ ¼å¼
                    md_content = f"## æ®µè½ {idx}\n\n{paragraph}"

                    metadata = {
                        "source": str(self.file_path.name),
                        "paragraph": idx,
                        "type": "doc",
                    }

                    if self.category:
                        metadata["category"] = self.category

                    doc = Document(page_content=md_content, metadata=metadata)
                    documents.append(doc)

            print(f"âœ… æå–å®Œæˆï¼Œå…±è·å– {len(documents)} ä¸ªæ®µè½")
            return documents

        except Exception as e:
            raise Exception(f"è¯»å– Word æ–‡æ¡£ï¼ˆDOCï¼‰å¤±è´¥: {e}\næç¤º: è¯·å®‰è£… antiword: sudo apt-get install antiword")


# ==================== DOCX åŠ è½½å™¨ï¼ˆå·²ä¼˜åŒ–ï¼‰====================

class DOCXLoader:
    """
    Word æ–‡æ¡£åŠ è½½å™¨ï¼ˆDOCXï¼‰
    æå–æ–‡æœ¬å†…å®¹å¹¶è½¬æ¢ä¸º Markdown æ ¼å¼ï¼Œä¿ç•™æ ‡é¢˜å±‚çº§ç»“æ„
    """

    def __init__(
        self,
        file_path: str | Path,
        category: Optional[Literal["policies", "cases"]] = None,
    ):
        self.file_path = Path(file_path)
        self.category = category
        self.cleaner = MarkdownCleaner()

    def load(self) -> List[Document]:
        """åŠ è½½ DOCX æ–‡ä»¶å¹¶è¿”å›æ–‡æ¡£åˆ—è¡¨"""
        if not self.file_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {self.file_path}")

        print(f"ğŸ“ æ­£åœ¨è¯»å– Word æ–‡æ¡£ï¼ˆDOCX æ ¼å¼ï¼‰: {self.file_path} ...")

        try:
            doc = DocxDocument(str(self.file_path))
            documents = []

            # æŒ‰æ®µè½æå–å†…å®¹
            current_content = []
            current_heading = "æ–‡æ¡£å¼€å§‹"
            current_level = 0
            paragraph_count = 0

            for para in doc.paragraphs:
                text = para.text.strip()

                if not text:
                    continue

                # æ£€æµ‹æ ‡é¢˜ï¼ˆWord å†…ç½®æ ·å¼ï¼‰
                style_name = para.style.name if para.style else ""

                if 'Heading' in style_name:
                    # ä¿å­˜ä¹‹å‰çš„å†…å®¹
                    if current_content:
                        content = '\n'.join(current_content).strip()
                        cleaned_content = self.cleaner.clean_text(content)

                        if self.cleaner.is_meaningful_content(cleaned_content):
                            md_content = f"{'#' * current_level} {current_heading}\n\n{cleaned_content}"

                            metadata = {
                                "source": str(self.file_path.name),
                                "paragraph": paragraph_count,
                                "type": "docx",
                            }

                            if self.category:
                                metadata["category"] = self.category

                            doc = Document(page_content=md_content, metadata=metadata)
                            documents.append(doc)

                    # å¼€å§‹æ–°æ ‡é¢˜
                    current_content = []
                    current_heading = text

                    # æå–æ ‡é¢˜çº§åˆ«ï¼ˆHeading 1 -> #, Heading 2 -> ##, etc.ï¼‰
                    if 'Heading 1' in style_name:
                        current_level = 1
                    elif 'Heading 2' in style_name:
                        current_level = 2
                    elif 'Heading 3' in style_name:
                        current_level = 3
                    elif 'Heading 4' in style_name:
                        current_level = 4
                    elif 'Heading 5' in style_name:
                        current_level = 5
                    else:
                        current_level = 6

                    paragraph_count += 1

                else:
                    # æ™®é€šæ®µè½
                    current_content.append(text)

            # ä¿å­˜æœ€åçš„å†…å®¹
            if current_content:
                content = '\n'.join(current_content).strip()
                cleaned_content = self.cleaner.clean_text(content)

                if self.cleaner.is_meaningful_content(cleaned_content):
                    md_content = f"{'#' * current_level} {current_heading}\n\n{cleaned_content}"

                    metadata = {
                        "source": str(self.file_path.name),
                        "paragraph": paragraph_count,
                        "type": "docx",
                    }

                    if self.category:
                        metadata["category"] = self.category

                    doc = Document(page_content=md_content, metadata=metadata)
                    documents.append(doc)

            # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°æ ‡é¢˜ï¼ŒæŒ‰æ®µè½åˆ‡åˆ†
            if not documents:
                paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]

                for idx, para_text in enumerate(paragraphs, start=1):
                    cleaned_text = self.cleaner.clean_text(para_text)

                    if self.cleaner.is_meaningful_content(cleaned_text):
                        md_content = f"## æ®µè½ {idx}\n\n{cleaned_text}"

                        metadata = {
                            "source": str(self.file_path.name),
                            "paragraph": idx,
                            "type": "docx",
                        }

                        if self.category:
                            metadata["category"] = self.category

                        doc = Document(page_content=md_content, metadata=metadata)
                        documents.append(doc)

            print(f"âœ… æå–å®Œæˆï¼Œå…±è·å– {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
            return documents

        except Exception as e:
            # å¦‚æœ python-docx å¤±è´¥ï¼Œå°è¯•é™çº§åˆ° DOCLoader
            print(f"âš ï¸  python-docx è¯»å–å¤±è´¥: {e}")
            print(f"âš ï¸  å¯èƒ½æ˜¯ä¼ªè£…æˆ .docx çš„ .doc æ–‡ä»¶ï¼Œå°è¯•ä½¿ç”¨ DOCLoader...")
            doc_loader = DOCLoader(self.file_path, self.category)
            return doc_loader.load()


# ==================== PDF åŠ è½½å™¨ ====================

class PDFLoader:
    """
    PDF æ–‡æ¡£åŠ è½½å™¨
    æå–æ–‡æœ¬å†…å®¹å¹¶è½¬æ¢ä¸º Markdown æ ¼å¼
    """

    def __init__(
        self,
        file_path: str | Path,
        category: Optional[Literal["policies", "cases"]] = None,
    ):
        self.file_path = Path(file_path)
        self.category = category
        self.cleaner = MarkdownCleaner()

    def load(self) -> List[Document]:
        """åŠ è½½ PDF æ–‡ä»¶å¹¶è¿”å›æ–‡æ¡£åˆ—è¡¨"""
        if not self.file_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {self.file_path}")

        print(f"ğŸ“„ æ­£åœ¨è¯»å– PDF: {self.file_path} ...")

        try:
            reader = PdfReader(str(self.file_path))
            documents = []

            # æŒ‰é¡µæå–æ–‡æœ¬
            for page_idx, page in enumerate(reader.pages, start=1):
                try:
                    text = page.extract_text()

                    if not text or not text.strip():
                        continue

                    # æ¸…ç†æ–‡æœ¬
                    cleaned_text = self.cleaner.clean_text(text)

                    # æ£€æŸ¥æ˜¯å¦æœ‰æ„ä¹‰
                    if not self.cleaner.is_meaningful_content(cleaned_text):
                        continue

                    # è½¬æ¢ä¸º Markdown æ ¼å¼
                    md_content = f"# ç¬¬ {page_idx} é¡µ\n\n{cleaned_text}"

                    # æ„é€ å…ƒæ•°æ®
                    metadata = {
                        "source": str(self.file_path.name),
                        "page": page_idx,
                        "type": "pdf",
                    }

                    if self.category:
                        metadata["category"] = self.category

                    doc = Document(page_content=md_content, metadata=metadata)
                    documents.append(doc)

                except Exception as e:
                    print(f"âš ï¸  å¤„ç†ç¬¬ {page_idx} é¡µæ—¶å‡ºé”™: {e}")
                    continue

            print(f"âœ… æå–å®Œæˆï¼Œå…±è·å– {len(documents)} é¡µæœ‰æ•ˆå†…å®¹")
            return documents

        except Exception as e:
            raise Exception(f"è¯»å– PDF æ–‡ä»¶å¤±è´¥: {e}")


# ==================== PPTX åŠ è½½å™¨ï¼ˆå·²ä¼˜åŒ–ï¼‰====================

class PPTXLoader:
    """
    PPTX æ–‡æ¡£åŠ è½½å™¨
    æå–æ–‡æœ¬å†…å®¹å¹¶è½¬æ¢ä¸º Markdown æ ¼å¼
    """

    def __init__(
        self,
        file_path: str | Path,
        category: Optional[Literal["policies", "cases"]] = None,
    ):
        self.file_path = Path(file_path)
        self.category = category
        self.cleaner = MarkdownCleaner()

    def load(self) -> List[Document]:
        """åŠ è½½ PPTX æ–‡ä»¶å¹¶è¿”å›æ–‡æ¡£åˆ—è¡¨"""
        if not self.file_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {self.file_path}")

        print(f"ğŸ“‚ æ­£åœ¨è¯»å– PPT: {self.file_path} ...")
        prs = Presentation(str(self.file_path))
        documents = []

        for slide_idx, slide in enumerate(prs.slides, start=1):
            slide_texts = []

            # æå–æ–‡æœ¬æ¡†å†…å®¹
            for shape in slide.shapes:
                if hasattr(shape, "text") and shape.text.strip():
                    text = shape.text.strip()
                    slide_texts.append(text)

            # åˆå¹¶å¹¶æ¸…ç†æ–‡æœ¬
            if slide_texts:
                content = "\n".join(slide_texts)
                cleaned_content = self.cleaner.clean_text(content)

                if self.cleaner.is_meaningful_content(cleaned_content):
                    # è½¬æ¢ä¸º Markdown æ ¼å¼
                    md_content = f"# ç¬¬ {slide_idx} é¡µ\n\n{cleaned_content}"

                    # æ„é€ å…ƒæ•°æ®
                    metadata = {
                        "source": str(self.file_path.name),
                        "page": slide_idx,
                        "type": "pptx",
                    }

                    if self.category:
                        metadata["category"] = self.category

                    doc = Document(page_content=md_content, metadata=metadata)
                    documents.append(doc)

        print(f"âœ… æå–å®Œæˆï¼Œå…±è·å– {len(documents)} é¡µæœ‰æ•ˆå†…å®¹")
        return documents


# ==================== Markdown åŠ è½½å™¨ï¼ˆä¿æŒä¸å˜ï¼‰====================

class MarkdownLoader:
    """
    Markdown æ–‡æ¡£åŠ è½½å™¨
    æŒ‰æ ‡é¢˜å±‚çº§åˆ†å‰² Markdown æ–‡ä»¶ï¼Œä¿ç•™ç»“æ„åŒ–ä¿¡æ¯
    """

    def __init__(
        self,
        file_path: str | Path,
        encoding: str = "utf-8",
        category: Optional[Literal["policies", "cases"]] = None,
    ):
        self.file_path = Path(file_path)
        self.encoding = encoding
        self.category = category
        self.cleaner = MarkdownCleaner()

    def load(self) -> List[Document]:
        """åŠ è½½ Markdown æ–‡ä»¶å¹¶è¿”å›æ–‡æ¡£åˆ—è¡¨"""
        if not self.file_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {self.file_path}")

        print(f"ğŸ“„ æ­£åœ¨è¯»å– Markdown: {self.file_path} ...")
        with open(self.file_path, "r", encoding=self.encoding, errors="ignore") as f:
            content = f.read()

        # æ¸…ç†å†…å®¹
        content = self.cleaner.clean_text(content)

        # æŒ‰æ ‡é¢˜åˆ†å‰²ï¼ˆæ”¯æŒ # ## ### ç­‰ï¼‰
        documents = self._split_by_headers(content)

        print(f"âœ… æå–å®Œæˆï¼Œå…±è·å– {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        return documents

    def _split_by_headers(self, content: str) -> List[Document]:
        """
        æŒ‰ Markdown æ ‡é¢˜åˆ†å‰²æ–‡æ¡£
        ä¿ç•™æ ‡é¢˜å±‚çº§å’Œç»“æ„
        """
        documents = []

        # æŒ‰è¡Œåˆ†å‰²
        lines = content.split("\n")

        current_section = []
        current_header = "æ–‡æ¡£å¼€å§‹"
        current_level = 0
        section_idx = 0

        for line in lines:
            # æ£€æµ‹æ ‡é¢˜ï¼ˆ# ## ### ç­‰ï¼‰
            header_match = re.match(r'^(#{1,6})\s+(.+)$', line)

            if header_match:
                # ä¿å­˜ä¹‹å‰çš„ç« èŠ‚
                if current_section:
                    section_content = "\n".join(current_section).strip()

                    if self.cleaner.is_meaningful_content(section_content):
                        metadata = self._build_metadata(
                            current_header, current_level, section_idx
                        )
                        doc = Document(page_content=section_content, metadata=metadata)
                        documents.append(doc)
                        section_idx += 1

                # å¼€å§‹æ–°ç« èŠ‚
                current_level = len(header_match.group(1))
                current_header = header_match.group(2).strip()
                current_section = [line]  # æ ‡é¢˜è¡Œä¹ŸåŒ…å«åœ¨å†…å®¹ä¸­
            else:
                current_section.append(line)

        # ä¿å­˜æœ€åä¸€ä¸ªç« èŠ‚
        if current_section:
            section_content = "\n".join(current_section).strip()
            if self.cleaner.is_meaningful_content(section_content):
                metadata = self._build_metadata(
                    current_header, current_level, section_idx
                )
                doc = Document(page_content=section_content, metadata=metadata)
                documents.append(doc)

        return documents

    def _build_metadata(
        self, header: str, level: int, section_idx: int
    ) -> Dict[str, any]:
        """æ„å»ºæ–‡æ¡£å…ƒæ•°æ®"""
        metadata = {
            "source": str(self.file_path.name),
            "section": section_idx + 1,
            "type": "markdown",
            "header": header,
            "header_level": level,
        }

        # å¦‚æœæŒ‡å®šäº†ç±»åˆ«ï¼Œæ·»åŠ åˆ°å…ƒæ•°æ®
        if self.category:
            metadata["category"] = self.category

        return metadata


# ==================== æ–‡æœ¬æ–‡ä»¶åŠ è½½å™¨ï¼ˆå·²ä¼˜åŒ–ï¼‰====================

class TextFileLoader:
    """
    TXT æ–‡æ¡£åŠ è½½å™¨
    æŒ‰æ®µè½åˆ†å‰²æ–‡æœ¬æ–‡ä»¶ï¼Œè½¬æ¢ä¸º Markdown æ ¼å¼
    """

    def __init__(
        self,
        file_path: str | Path,
        encoding: str = "utf-8",
        category: Optional[Literal["policies", "cases"]] = None,
    ):
        self.file_path = Path(file_path)
        self.encoding = encoding
        self.category = category
        self.cleaner = MarkdownCleaner()

    def load(self) -> List[Document]:
        """åŠ è½½æ–‡æœ¬æ–‡ä»¶å¹¶è¿”å›æ–‡æ¡£åˆ—è¡¨"""
        if not self.file_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {self.file_path}")

        print(f"ğŸ“‚ æ­£åœ¨è¯»å–æ–‡æœ¬æ–‡ä»¶: {self.file_path} ...")
        with open(self.file_path, "r", encoding=self.encoding, errors="ignore") as f:
            content = f.read()

        # æ¸…ç†æ–‡æœ¬
        content = self.cleaner.clean_text(content)

        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in content.split("\n\n") if p.strip()]

        documents = []
        for idx, paragraph in enumerate(paragraphs, start=1):
            if self.cleaner.is_meaningful_content(paragraph):
                # è½¬æ¢ä¸º Markdown æ ¼å¼
                md_content = f"## æ®µè½ {idx}\n\n{paragraph}"

                metadata = {
                    "source": str(self.file_path.name),
                    "paragraph": idx,
                    "type": "text",
                }

                # å¦‚æœæŒ‡å®šäº†ç±»åˆ«ï¼Œæ·»åŠ åˆ°å…ƒæ•°æ®
                if self.category:
                    metadata["category"] = self.category

                doc = Document(page_content=md_content, metadata=metadata)
                documents.append(doc)

        print(f"âœ… æå–å®Œæˆï¼Œå…±è·å– {len(documents)} ä¸ªæ®µè½")
        return documents


# ==================== æ‰¹é‡åŠ è½½å‡½æ•°ï¼ˆå·²ä¼˜åŒ–ï¼‰====================

def load_documents_from_directory(
    directory: str | Path,
    file_extensions: Optional[List[str]] = None,
    category: Optional[Literal["policies", "cases"]] = None,
) -> List[Document]:
    """
    ä»ç›®å½•æ‰¹é‡åŠ è½½æ–‡æ¡£
    è‡ªåŠ¨æ£€æµ‹çœŸå®æ–‡ä»¶ç±»å‹ï¼Œä¸ä¾èµ–æ‰©å±•å

    Args:
        directory: æ–‡æ¡£ç›®å½•è·¯å¾„
        file_extensions: è¦åŠ è½½çš„æ–‡ä»¶æ‰©å±•ååˆ—è¡¨ï¼Œå¦‚ [".md", ".txt", ".pptx", ".pdf", ".docx", ".doc"]
        category: æ–‡æ¡£ç±»åˆ«ï¼ˆ"policies" æˆ– "cases"ï¼‰ï¼Œä¼šæ·»åŠ åˆ°å…ƒæ•°æ®ä¸­

    Returns:
        æ‰€æœ‰æ–‡æ¡£çš„åˆ—è¡¨
    """
    directory = Path(directory)
    if not directory.exists():
        raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨: {directory}")

    if file_extensions is None:
        file_extensions = [".md", ".txt", ".pptx", ".ppt", ".pdf", ".docx", ".doc"]

    all_documents = []

    # éå†ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
    for file_path in directory.rglob("*"):
        if file_path.is_file() and file_path.suffix.lower() in file_extensions:
            try:
                # æ£€æµ‹çœŸå®æ–‡ä»¶ç±»å‹
                real_type = FileTypeDetector.detect(file_path)

                print(f"ğŸ” æ£€æµ‹æ–‡ä»¶ç±»å‹: {file_path.name} -> {real_type}")

                # æ ¹æ®çœŸå®ç±»å‹é€‰æ‹©åŠ è½½å™¨
                if real_type == 'pdf':
                    loader = PDFLoader(file_path, category=category)
                elif real_type == 'doc':
                    loader = DOCLoader(file_path, category=category)
                elif real_type == 'docx':
                    loader = DOCXLoader(file_path, category=category)
                elif real_type == 'ppt':
                    print(f"âš ï¸  æš‚ä¸æ”¯æŒ PPT æ ¼å¼ï¼Œè¯·è½¬æ¢ä¸º PPTX: {file_path.name}")
                    continue
                elif real_type == 'pptx':
                    loader = PPTXLoader(file_path, category=category)
                elif real_type == 'markdown':
                    loader = MarkdownLoader(file_path, category=category)
                elif real_type == 'txt':
                    loader = TextFileLoader(file_path, category=category)
                else:
                    print(f"âš ï¸  ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {real_type}ï¼Œè·³è¿‡: {file_path.name}")
                    continue

                documents = loader.load()
                all_documents.extend(documents)

            except Exception as e:
                print(f"âš ï¸  åŠ è½½æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
                continue

    print(f"ğŸ“š æ€»å…±åŠ è½½äº† {len(all_documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
    return all_documents


def load_knowledge_base(
    data_dir: str | Path,
    categories: Optional[List[Literal["policies", "cases"]]] = None,
) -> List[Document]:
    """
    åŠ è½½çŸ¥è¯†åº“ï¼ˆæ”¯æŒåˆ†ç±»ï¼‰
    è‡ªåŠ¨æ£€æµ‹çœŸå®æ–‡ä»¶ç±»å‹ï¼Œä¸ä¾èµ–æ‰©å±•å

    ç›®å½•ç»“æ„:
    data/
    â”œâ”€â”€ policies/
    â”‚   â”œâ”€â”€ æ–‡ä»¶1.md
    â”‚   â””â”€â”€ æ–‡ä»¶2.pdfï¼ˆå¯èƒ½æ˜¯ .doc ä¼ªè£…çš„ï¼‰
    â””â”€â”€ cases/
        â”œâ”€â”€ æ¡ˆä¾‹1.docx
        â””â”€â”€ æ¡ˆä¾‹2.pptx

    Args:
        data_dir: æ•°æ®æ ¹ç›®å½•ï¼ˆé»˜è®¤ä¸º src/dataï¼‰
        categories: è¦åŠ è½½çš„ç±»åˆ«åˆ—è¡¨ï¼Œå¦‚ ["policies", "cases"]ã€‚
                     å¦‚æœä¸º Noneï¼Œåˆ™åŠ è½½æ‰€æœ‰ç±»åˆ«ã€‚

    Returns:
        æ‰€æœ‰æ–‡æ¡£çš„åˆ—è¡¨
    """
    data_dir = Path(data_dir)

    if categories is None:
        # è‡ªåŠ¨æ£€æµ‹æ‰€æœ‰å­ç›®å½•ä½œä¸ºç±»åˆ«
        categories = []
        for item in data_dir.iterdir():
            if item.is_dir() and item.name in ["policies", "cases"]:
                categories.append(item.name)

    if not categories:
        raise FileNotFoundError(
            f"æœªæ‰¾åˆ°ä»»ä½•ç±»åˆ«ç›®å½•ã€‚è¯·åœ¨ {data_dir} ä¸‹åˆ›å»º 'policies' å’Œ/æˆ– 'cases' ç›®å½•ã€‚"
        )

    all_documents = []

    for category in categories:
        category_dir = data_dir / category

        if not category_dir.exists():
            print(f"âš ï¸  ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡: {category_dir}")
            continue

        print(f"\n{'='*60}")
        print(f"æ­£åœ¨åŠ è½½ç±»åˆ«: {category}")
        print(f"{'='*60}")

        documents = load_documents_from_directory(
            category_dir,
            file_extensions=[".md", ".txt", ".pptx", ".ppt", ".pdf", ".docx", ".doc"],
            category=category,  # æ·»åŠ ç±»åˆ«æ ‡è®°åˆ°å…ƒæ•°æ®
        )

        all_documents.extend(documents)

    print(f"\n{'='*60}")
    print(f"âœ… çŸ¥è¯†åº“åŠ è½½å®Œæˆï¼")
    print(f"   - æ€»æ–‡æ¡£æ•°: {len(all_documents)}")
    print(f"   - ç±»åˆ«: {', '.join(categories)}")
    print(f"{'='*60}\n")

    return all_documents
