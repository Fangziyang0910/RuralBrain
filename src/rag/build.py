"""
çŸ¥è¯†åº“æ„å»ºè„šæœ¬
ç¬¦åˆ LangChain æœ€ä½³å®è·µï¼Œæ”¯æŒ Docker éƒ¨ç½²
é’ˆå¯¹ Planning Agent ä¼˜åŒ–ï¼šæ›´å¤§çš„ chunk_size ä¿ç•™ä¸Šä¸‹æ–‡
"""
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

from src.rag.config import (
    CHUNK_OVERLAP,
    CHUNK_SIZE,
    CHROMA_COLLECTION_NAME,
    CHROMA_PERSIST_DIR,
    DATA_DIR,
    EMBEDDING_MODEL_NAME,
    EMBEDDING_DEVICE,
    VECTOR_DB_TYPE,
    DEFAULT_PROVIDER,
    is_docker,
)
from src.rag.utils import load_knowledge_base
from src.rag.visualize import SliceInspector
from src.rag.core.context_manager import DocumentContextManager
from src.rag.core.summarization import DocumentSummarizer, DocumentSummary


def load_documents():
    """
    åŠ è½½æ–‡æ¡£ï¼ˆæ”¯æŒåˆ†ç±»ï¼‰
    ç›®å½•ç»“æ„:
    src/data/
    â”œâ”€â”€ policies/
    â”‚   â”œâ”€â”€ *.md
    â”‚   â”œâ”€â”€ *.txt
    â”‚   â”œâ”€â”€ *.pptx
    â”‚   â”œâ”€â”€ *.pdf
    â”‚   â””â”€â”€ *.docx
    â””â”€â”€ cases/
        â”œâ”€â”€ *.md
        â”œâ”€â”€ *.txt
        â”œâ”€â”€ *.pptx
        â”œâ”€â”€ *.pdf
        â””â”€â”€ *.docx
    """
    if not DATA_DIR.exists():
        print(f"âŒ é”™è¯¯ï¼šæ•°æ®ç›®å½•ä¸å­˜åœ¨: {DATA_DIR}")
        print(f"\nè¯·æŒ‰ä»¥ä¸‹ç»“æ„ç»„ç»‡æ•°æ®:")
        print(f"  {DATA_DIR}/")
        print(f"  â”œâ”€â”€ policies/")
        print(f"  â”‚   â”œâ”€â”€ æ–‡ä»¶1.md")
        print(f"  â”‚   â”œâ”€â”€ æ–‡ä»¶2.pdf")
        print(f"  â”‚   â””â”€â”€ æ–‡ä»¶3.docx")
        print(f"  â””â”€â”€ cases/")
        print(f"      â”œâ”€â”€ æ¡ˆä¾‹1.md")
        print(f"      â”œâ”€â”€ æ¡ˆä¾‹2.pptx")
        print(f"      â””â”€â”€ æ¡ˆä¾‹3.txt")
        return []

    # ä½¿ç”¨æ–°çš„åˆ†ç±»åŠ è½½å‡½æ•°
    try:
        documents = load_knowledge_base(DATA_DIR)
        return documents
    except FileNotFoundError as e:
        print(f"âŒ {e}")
        return []


def split_documents(documents):
    """
    åˆ‡åˆ†æ–‡æ¡£
    é’ˆå¯¹ Planning Agent ä¼˜åŒ–ï¼šä½¿ç”¨æ›´å¤§çš„ chunk_size
    """
    print("\nâœ‚ï¸  æ­£åœ¨åˆ‡åˆ†æ–‡æ¡£...")
    print(f"   é…ç½®: chunk_size={CHUNK_SIZE}, chunk_overlap={CHUNK_OVERLAP}")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        add_start_index=True,  # æ·»åŠ å­—ç¬¦ç´¢å¼•ç”¨äºå¼•ç”¨
    )

    splits = text_splitter.split_documents(documents)
    print(f"âœ… åˆ‡åˆ†å®Œæˆï¼Œå…± {len(splits)} ä¸ªåˆ‡ç‰‡")

    return splits


def visualize_splits(splits):
    """
    å¯è§†åŒ–åˆ‡ç‰‡ç»“æœ
    å¸®åŠ©å‘ç°å†—ä½™å’Œåƒåœ¾ä¿¡æ¯
    """
    print("\nğŸ“Š åˆ‡ç‰‡å¯è§†åŒ–åˆ†æ\n")

    inspector = SliceInspector(splits)

    # æ‰“å°ç»Ÿè®¡æ‘˜è¦
    inspector.print_summary()

    # æ‰“å°å‰ 3 ä¸ªåˆ‡ç‰‡çš„è¯¦æƒ…
    print("\n" + "="*60)
    inspector.print_slice_details(start_idx=0, end_idx=3, show_content=True)

    # æŸ¥æ‰¾å¹¶æ‰“å°æ½œåœ¨é—®é¢˜
    print("\n" + "="*60)
    inspector.print_issues(max_issues=10)

    # å¯¼å‡ºå®Œæ•´æ•°æ®åˆ° JSONï¼ˆå¯é€‰ï¼‰
    output_json = CHROMA_PERSIST_DIR / "slices_analysis.json"
    inspector.export_to_json(output_json)

    return inspector


def build_vector_store(splits):
    """
    æ„å»ºå‘é‡å­˜å‚¨
    æ”¯æŒå¤šç§å‘é‡æ•°æ®åº“ï¼ˆChroma/FAISS/Qdrantï¼‰
    """
    print(f"\nğŸ§  æ­£åœ¨åˆå§‹åŒ– Embedding æ¨¡å‹: {EMBEDDING_MODEL_NAME}")

    # æ£€æµ‹æ˜¯å¦åœ¨ Docker ä¸­è¿è¡Œ
    if is_docker():
        print("ğŸ³ æ£€æµ‹åˆ° Docker ç¯å¢ƒï¼Œä½¿ç”¨ CPU æ¨ç†")
        device = "cpu"
    else:
        device = EMBEDDING_DEVICE
        print(f"ğŸ’» è®¾å¤‡: {device}")

    # åˆå§‹åŒ– Embedding æ¨¡å‹
    embedding_model = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={"device": device},
        encode_kwargs={"normalize_embeddings": True},  # å½’ä¸€åŒ–å‘é‡
    )

    # æ ¹æ®é…ç½®é€‰æ‹©å‘é‡æ•°æ®åº“
    if VECTOR_DB_TYPE == "chroma":
        print(f"ğŸ’¾ ä½¿ç”¨ Chroma å‘é‡æ•°æ®åº“")
        print(f"   æŒä¹…åŒ–è·¯å¾„: {CHROMA_PERSIST_DIR}")

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        CHROMA_PERSIST_DIR.mkdir(parents=True, exist_ok=True)

        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=embedding_model,
            collection_name=CHROMA_COLLECTION_NAME,
            persist_directory=str(CHROMA_PERSIST_DIR),
        )

        print(f"âœ… Chroma æ•°æ®åº“æ„å»ºå®Œæˆï¼")
        print(f"   é›†åˆåç§°: {CHROMA_COLLECTION_NAME}")

        return vectorstore

    elif VECTOR_DB_TYPE == "faiss":
        print("ğŸ’¾ ä½¿ç”¨ FAISS å‘é‡æ•°æ®åº“ï¼ˆæš‚æœªå®ç°ï¼‰")
        raise NotImplementedError("FAISS æ”¯æŒå³å°†æ¨å‡º")

    elif VECTOR_DB_TYPE == "qdrant":
        print("ğŸ’¾ ä½¿ç”¨ Qdrant å‘é‡æ•°æ®åº“ï¼ˆæš‚æœªå®ç°ï¼‰")
        raise NotImplementedError("Qdrant æ”¯æŒå³å°†æ¨å‡º")

    else:
        raise ValueError(f"ä¸æ”¯æŒçš„å‘é‡æ•°æ®åº“ç±»å‹: {VECTOR_DB_TYPE}")


def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ğŸš€ å¼€å§‹æ„å»ºçŸ¥è¯†åº“ï¼ˆPlanning Agent ä¼˜åŒ–ç‰ˆï¼‰")
    print("="*60)

    # 1. åŠ è½½æ–‡æ¡£
    documents = load_documents()
    if not documents:
        print("\nâŒ æ²¡æœ‰åŠ è½½åˆ°æ–‡æ¡£ï¼Œé€€å‡ºæ„å»º")
        return

    # 2. åˆ‡åˆ†æ–‡æ¡£
    splits = split_documents(documents)

    # 3. å¯è§†åŒ–åˆ‡ç‰‡ï¼ˆå¸®åŠ©å‘ç°é—®é¢˜å’Œä¼˜åŒ–ï¼‰
    inspector = visualize_splits(splits)

    # è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
    print("\n" + "="*60)
    user_input = input("æ˜¯å¦ç»§ç»­æ„å»ºå‘é‡æ•°æ®åº“ï¼Ÿ(y/n): ").strip().lower()
    if user_input not in ['y', 'yes', 'æ˜¯']:
        print("âŒ å·²å–æ¶ˆæ„å»º")
        return

    # 4. æ„å»ºå‘é‡å­˜å‚¨
    print("\nğŸ”¨ å¼€å§‹æ„å»ºå‘é‡æ•°æ®åº“...")
    vectorstore = build_vector_store(splits)

    # 5. æ„å»ºå¹¶ä¿å­˜æ–‡æ¡£ç´¢å¼•ï¼ˆç”¨äºä¸Šä¸‹æ–‡ç®¡ç†ï¼‰
    print("\nğŸ“š æ­£åœ¨æ„å»ºæ–‡æ¡£ç´¢å¼•ï¼ˆæ”¯æŒå…¨æ–‡ä¸Šä¸‹æ–‡æŸ¥è¯¢ï¼‰...")
    context_manager = DocumentContextManager()
    context_manager.build_index(documents, splits)

    # é˜¶æ®µ2æ–°å¢ï¼šç”Ÿæˆæ–‡æ¡£æ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ“ é˜¶æ®µ2ï¼šç”Ÿæˆå±‚æ¬¡åŒ–æ‘˜è¦ï¼ˆå¯é€‰ï¼‰")
    print("="*60)
    print("æç¤ºï¼šæ‘˜è¦ç”Ÿæˆéœ€è¦è°ƒç”¨ LLM APIï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´")
    print("      å¦‚æœä¸éœ€è¦æ‘˜è¦åŠŸèƒ½ï¼Œå¯ä»¥è·³è¿‡æ­¤æ­¥éª¤\n")

    user_input = input("æ˜¯å¦ç”Ÿæˆæ–‡æ¡£æ‘˜è¦ï¼Ÿï¼ˆæ¨èï¼‰(y/n): ").strip().lower()

    if user_input in ['y', 'yes', 'æ˜¯']:
        print("\nâ³ æ­£åœ¨ç”Ÿæˆæ–‡æ¡£æ‘˜è¦...")
        print(f"   æ¨¡å‹: {DEFAULT_PROVIDER}")
        print(f"   æ–‡æ¡£æ•°: {len(documents)}\n")

        try:
            # åˆå§‹åŒ–æ‘˜è¦ç”Ÿæˆå™¨
            summarizer = DocumentSummarizer(provider=DEFAULT_PROVIDER)

            # ä¸ºæ¯ä¸ªæ–‡æ¡£ç”Ÿæˆæ‘˜è¦
            summary_count = 0
            for doc in documents:
                source = doc.metadata.get("source", "unknown")

                # æ£€æŸ¥è¯¥æ–‡æ¡£æ˜¯å¦å·²åœ¨ç´¢å¼•ä¸­
                if source in context_manager.doc_index:
                    try:
                        print(f"   [{summary_count + 1}/{len(documents)}] ç”Ÿæˆæ‘˜è¦: {source}")
                        summary = summarizer.generate_summary(doc)

                        # æ›´æ–°ç´¢å¼•ä¸­çš„æ‘˜è¦å­—æ®µ
                        doc_index = context_manager.doc_index[source]
                        doc_index.executive_summary = summary.executive_summary
                        doc_index.chapter_summaries = [
                            {
                                "title": ch.title,
                                "level": ch.level,
                                "summary": ch.summary,
                                "key_points": ch.key_points,
                                "start_index": ch.start_index,
                                "end_index": ch.end_index
                            }
                            for ch in summary.chapter_summaries
                        ]
                        doc_index.key_points = summary.key_points

                        summary_count += 1

                    except Exception as e:
                        print(f"   âš ï¸  {source} æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}")
                        continue

            print(f"\nâœ… æ‘˜è¦ç”Ÿæˆå®Œæˆ: {summary_count}/{len(documents)} ä¸ªæ–‡æ¡£")

        except Exception as e:
            print(f"\nâŒ æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}")
            print("   ç´¢å¼•å°†ä¸åŒ…å«æ‘˜è¦ä¿¡æ¯ï¼Œä½†ä»å¯æ­£å¸¸ä½¿ç”¨")
            import traceback
            traceback.print_exc()

    else:
        print("â­ï¸  å·²è·³è¿‡æ‘˜è¦ç”Ÿæˆ")
        print("   æç¤ºï¼šå¯ä»¥ç¨åè¿è¡Œ build.py é‡æ–°ç”Ÿæˆæ‘˜è¦")

    # ä¿å­˜ç´¢å¼•ï¼ˆæ— è®ºæ˜¯å¦ç”Ÿæˆæ‘˜è¦ï¼‰
    context_manager.save()
    print(f"âœ… æ–‡æ¡£ç´¢å¼•å·²ä¿å­˜")

    # 6. å®Œæˆ
    print("\n" + "="*60)
    print("ğŸ‰ çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")
    print("="*60)
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   â€¢ åŸå§‹æ–‡æ¡£æ•°: {len(documents)}")
    print(f"   â€¢ åˆ‡ç‰‡æ•°é‡: {len(splits)}")
    print(f"   â€¢ å¹³å‡åˆ‡ç‰‡å¤§å°: {inspector.stats['avg_chars']:.0f} å­—ç¬¦")
    print(f"\nğŸ’¾ æ•°æ®åº“ä½ç½®: {CHROMA_PERSIST_DIR}")
    print(f"ğŸ“Š åˆ‡ç‰‡åˆ†ææŠ¥å‘Š: {CHROMA_PERSIST_DIR / 'slices_analysis.json'}")
    print(f"ğŸ“– æ–‡æ¡£ç´¢å¼•: {CHROMA_PERSIST_DIR / 'document_index.json'}")
    print(f"\nâœ… å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä½¿ç”¨çŸ¥è¯†åº“:")
    print(f"   from src.rag.core.tools import planning_knowledge_tool")
    print(f"   planning_knowledge_tool.run('ä½ çš„é—®é¢˜')")
    print(f"\nâœ… é˜¶æ®µ1å·¥å…·ï¼ˆå…¨æ–‡ä¸Šä¸‹æ–‡æŸ¥è¯¢ï¼‰:")
    print(f"   cm.get_full_document('æ–‡ä»¶å')")
    print(f"   cm.get_chapter_by_header('æ–‡ä»¶å', 'ç« èŠ‚å…³é”®è¯')")
    print(f"\nâœ… é˜¶æ®µ2å·¥å…·ï¼ˆå±‚æ¬¡åŒ–æ‘˜è¦ï¼Œå¦‚æœå·²ç”Ÿæˆï¼‰:")
    print(f"   cm.get_executive_summary('æ–‡ä»¶å')")
    print(f"   cm.list_chapter_summaries('æ–‡ä»¶å')")
    print(f"   cm.get_chapter_summary('æ–‡ä»¶å', 'ç« èŠ‚å…³é”®è¯')")
    print(f"   cm.search_key_points('å…³é”®è¯')")


if __name__ == "__main__":
    main()
