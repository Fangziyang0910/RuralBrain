"""
çŸ¥è¯†åº“æ£€ç´¢å·¥å…·ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
åŸºäº references/agent_skills æœ€ä½³å®è·µé‡æ„

æ ¸å¿ƒæ”¹è¿›ï¼š
1. åº”ç”¨ Consolidation Principle - ä» 10+ ä¸ªå·¥å…·ç²¾ç®€åˆ° 7 ä¸ªæ ¸å¿ƒå·¥å…·
2. ä¼˜åŒ–å·¥å…·æè¿° - éµå¾ª"åšä»€ä¹ˆã€ä½•æ—¶ç”¨ã€è¿”å›ä»€ä¹ˆ"åŸåˆ™
3. ç»Ÿä¸€å‚æ•°æ ¼å¼ - ä½¿ç”¨ä¸€è‡´çš„è®¾è®¡æ¨¡å¼
4. æ”¯æŒæ¸è¿›å¼æŠ«éœ² - é€šè¿‡å‚æ•°æ§åˆ¶è¿”å›è¯¦ç»†ç¨‹åº¦
"""
import os
from pathlib import Path
from typing import List, Optional

from langchain_core.documents import Document
from langchain_core.tools import Tool
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# å¯¼å…¥é…ç½®
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.rag.config import (
    CHROMA_COLLECTION_NAME,
    CHROMA_PERSIST_DIR,
    DEFAULT_TOP_K,
    EMBEDDING_MODEL_NAME,
)
from src.rag.core.context_manager import get_context_manager
from src.rag.core.cache import get_vector_cache


def get_vectorstore():
    """
    è·å–å‘é‡æ•°æ®åº“ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰

    ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨ VectorStoreCache æ›¿ä»£æ‡’åŠ è½½å…¨å±€å˜é‡
    - Embedding æ¨¡å‹è‡ªåŠ¨ç¼“å­˜
    - å‘é‡æ•°æ®åº“è¿æ¥è‡ªåŠ¨ç¼“å­˜
    - æ”¯æŒæŸ¥è¯¢ç»“æœç¼“å­˜ï¼ˆå¯é€‰ï¼‰
    """
    cache = get_vector_cache()
    return cache.get_vectorstore()


# ==================== å·¥å…· 1ï¼šåˆ—å‡ºå¯ç”¨æ–‡æ¡£ ====================

def list_available_documents(query: str = "") -> str:
    """
    åˆ—å‡ºçŸ¥è¯†åº“ä¸­æ‰€æœ‰å¯ç”¨æ–‡æ¡£

    **ä½•æ—¶ä½¿ç”¨ï¼š**
    - ä»»åŠ¡å¼€å§‹æ—¶ï¼Œäº†è§£æœ‰å“ªäº›èµ„æ–™
    - ç”¨æˆ·è¯¢é—®"ä½ æœ‰ä»€ä¹ˆçŸ¥è¯†åº“"ã€"ä½ èƒ½åšä»€ä¹ˆ"æ—¶

    **è¿”å›ï¼š**
    - æ–‡æ¡£åç§°ã€ç±»å‹ã€åˆ‡ç‰‡æ•°é‡ã€å†…å®¹é¢„è§ˆ

    **ç¤ºä¾‹ï¼š**
    - "ä½ æœ‰ä»€ä¹ˆæ–‡æ¡£ï¼Ÿ"
    - "çŸ¥è¯†åº“é‡Œæœ‰å“ªäº›èµ„æ–™ï¼Ÿ"
    """
    try:
        cm = get_context_manager()
        cm._ensure_loaded()

        if not cm.doc_index:
            return "âš ï¸  çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡æ¡£"

        output = ["ã€å¯ç”¨æ–‡æ¡£åˆ—è¡¨ã€‘\n"]

        for idx, (source, doc_idx) in enumerate(cm.doc_index.items(), 1):
            output.append(
                f"{idx}. {source}\n"
                f"   ç±»å‹: {doc_idx.doc_type}\n"
                f"   åˆ‡ç‰‡æ•°: {len(doc_idx.chunks_info)}\n"
                f"   é¢„è§ˆ: {doc_idx.chunks_info[0]['content_preview'] if doc_idx.chunks_info else 'N/A'}\n"
            )

        return "\n".join(output)

    except Exception as e:
        return f"âŒ åˆ—å‡ºæ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"


# ==================== å·¥å…· 2ï¼šè·å–æ–‡æ¡£æ¦‚è§ˆï¼ˆæ–°å¢ï¼‰====================

def get_document_overview(source: str, include_chapters: bool = True) -> str:
    """
    è·å–æ–‡æ¡£æ¦‚è§ˆï¼ˆæ‰§è¡Œæ‘˜è¦ + å¯é€‰ç« èŠ‚åˆ—è¡¨ï¼‰

    **åŠŸèƒ½ï¼š**
    å¿«é€Ÿäº†è§£æ–‡æ¡£æ ¸å¿ƒå†…å®¹ï¼ŒåŒ…å« 200 å­—æ‰§è¡Œæ‘˜è¦å’Œå¯é€‰çš„ç« èŠ‚åˆ—è¡¨

    **ä½•æ—¶ä½¿ç”¨ï¼š**
    - å¿«é€Ÿäº†è§£æ–‡æ¡£æ ¸å¿ƒå†…å®¹
    - å†³å®šæ˜¯å¦éœ€è¦æ·±å…¥é˜…è¯»
    - å¯¹æ¯”å¤šä¸ªæ–‡æ¡£çš„ä¸»é¢˜

    **å‚æ•°ï¼š**
    - source (str | required): æ–‡æ¡£åç§°ï¼ˆæ–‡ä»¶åï¼‰
    - include_chapters (bool | optional): æ˜¯å¦åŒ…å«ç« èŠ‚åˆ—è¡¨ï¼Œé»˜è®¤ True

    **è¿”å›ï¼š**
    - æ‰§è¡Œæ‘˜è¦ï¼ˆ200 å­—ï¼‰
    - ç« èŠ‚æ ‡é¢˜åˆ—è¡¨ï¼ˆå¦‚æœ include_chapters=Trueï¼‰

    **ç¤ºä¾‹ï¼š**
    - get_document_overview("ç½—æµ®-é•¿å®å±±é•‡èåˆå‘å±•æˆ˜ç•¥.pptx")
    - get_document_overview("plan.docx", include_chapters=False)

    **æ³¨æ„ï¼š**
    - å¦‚æœæ–‡æ¡£å°šæœªç”Ÿæˆæ‘˜è¦ï¼Œä¼šæç¤ºç”¨æˆ·å…ˆè¿è¡ŒçŸ¥è¯†åº“æ„å»º
    """
    try:
        cm = get_context_manager()
        result = cm.get_executive_summary(source)

        if "error" in result:
            return f"âŒ {result['error']}"

        output = [
            f"ã€æ–‡æ¡£æ¦‚è§ˆã€‘\n",
            f"æ¥æº: {result['source']}\n",
            f"ç±»å‹: {result['doc_type']}\n\n",
        ]

        # æ·»åŠ æ‰§è¡Œæ‘˜è¦
        if result.get("executive_summary"):
            output.append(f"**æ‰§è¡Œæ‘˜è¦**\n{result['executive_summary']}\n")
        else:
            output.append(f"âš ï¸  è¯¥æ–‡æ¡£å°šæœªç”Ÿæˆæ‘˜è¦\n")

        # æ·»åŠ ç« èŠ‚åˆ—è¡¨
        if include_chapters:
            chapters_result = cm.list_chapter_summaries(source)
            if chapters_result.get("chapters"):
                output.append(f"\n**ç« èŠ‚åˆ—è¡¨**\n")
                for idx, chapter in enumerate(chapters_result['chapters'], 1):
                    output.append(f"{idx}. {chapter['title']}\n")

        return "\n".join(output)

    except Exception as e:
        return f"âŒ è·å–æ–‡æ¡£æ¦‚è§ˆæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"


# ==================== å·¥å…· 3ï¼šè·å–ç« èŠ‚å†…å®¹ï¼ˆæ–°å¢ï¼‰====================

def get_chapter_content(source: str, chapter_pattern: str, detail_level: str = "medium") -> str:
    """
    è·å–ç« èŠ‚å†…å®¹ï¼ˆæ”¯æŒä¸‰çº§è¯¦æƒ…ï¼‰

    **åŠŸèƒ½ï¼š**
    æ ¹æ®éœ€æ±‚è·å–ä¸åŒè¯¦ç»†ç¨‹åº¦çš„ç« èŠ‚å†…å®¹ï¼Œä»æ‘˜è¦åˆ°å®Œæ•´å†…å®¹

    **ä½•æ—¶ä½¿ç”¨ï¼š**
    - äº†è§£ç‰¹å®šç« èŠ‚å†…å®¹æ—¶
    - æ ¹æ®ä¿¡æ¯éœ€æ±‚é€‰æ‹©åˆé€‚çš„è¯¦ç»†ç¨‹åº¦
    - å¿«é€Ÿæµè§ˆæˆ–æ·±åº¦é˜…è¯»ç‰¹å®šç« èŠ‚

    **å‚æ•°ï¼š**
    - source (str | required): æ–‡æ¡£åç§°ï¼ˆæ–‡ä»¶åï¼‰
    - chapter_pattern (str | required): ç« èŠ‚æ ‡é¢˜å…³é”®è¯ï¼ˆæ”¯æŒéƒ¨åˆ†åŒ¹é…ï¼‰
    - detail_level (str | optional): è¯¦ç»†ç¨‹åº¦
      - "summary": ä»…æ‘˜è¦ï¼ˆ100-200 å­—ï¼‰- æœ€å¿«
      - "medium": æ‘˜è¦ + å…³é”®è¦ç‚¹ï¼ˆé»˜è®¤ï¼‰
      - "full": å®Œæ•´ç« èŠ‚å†…å®¹ - æœ€è¯¦ç»†

    **è¿”å›ï¼š**
    - detail_level="summary": ç« èŠ‚æ‘˜è¦
    - detail_level="medium": ç« èŠ‚æ‘˜è¦ + å…³é”®è¦ç‚¹
    - detail_level="full": å®Œæ•´ç« èŠ‚å†…å®¹

    **ç¤ºä¾‹ï¼š**
    - å¿«é€Ÿæµè§ˆï¼šget_chapter_content("plan.docx", "ç¬¬ä¸€ç« ", "summary")
    - ä¸­ç­‰æ·±åº¦ï¼šget_chapter_content("plan.docx", "äº§ä¸š", "medium")
    - æ·±åº¦é˜…è¯»ï¼šget_chapter_content("plan.docx", "æŠ•èµ„", "full")

    **æ³¨æ„ï¼š**
    - æ”¯æŒæ ‡é¢˜çš„éƒ¨åˆ†åŒ¹é…ï¼Œä¸å¿…è¾“å…¥å®Œæ•´æ ‡é¢˜
    - "full" æ¨¡å¼å¯èƒ½è¿”å›è¾ƒé•¿å†…å®¹ï¼Œè°¨æ…ä½¿ç”¨
    """
    try:
        cm = get_context_manager()

        # æ ¹æ®è¯¦ç»†ç¨‹åº¦é€‰æ‹©ä¸åŒçš„æ–¹æ³•
        if detail_level == "summary":
            # ä»…è¿”å›æ‘˜è¦
            result = cm.get_chapter_summary(source, chapter_pattern)
            if "error" in result:
                return f"âŒ {result['error']}"

            output = [
                f"ã€ç« èŠ‚æ‘˜è¦ã€‘\n",
                f"æ¥æº: {result['source']}\n",
                f"ç« èŠ‚: {result['chapter_title']}\n\n",
                f"{result['summary']}"
            ]
            return "\n".join(output)

        elif detail_level == "medium":
            # è¿”å›æ‘˜è¦ + è¦ç‚¹
            result = cm.get_chapter_summary(source, chapter_pattern)
            if "error" in result:
                return f"âŒ {result['error']}"

            output = [
                f"ã€ç« èŠ‚å†…å®¹ï¼ˆä¸­ç­‰è¯¦ç»†ï¼‰ã€‘\n",
                f"æ¥æº: {result['source']}\n",
                f"ç« èŠ‚: {result['chapter_title']}\n\n",
                f"**æ‘˜è¦**\n{result['summary']}\n\n",
                f"**å…³é”®è¦ç‚¹**\n"
            ]

            for point in result.get('key_points', []):
                output.append(f"  â€¢ {point}")

            return "\n".join(output)

        elif detail_level == "full":
            # è¿”å›å®Œæ•´ç« èŠ‚
            result = cm.get_chapter_by_header(source, chapter_pattern)
            if "error" in result:
                return f"âŒ {result['error']}"

            output = [
                f"ã€ç« èŠ‚å®Œæ•´å†…å®¹ã€‘\n",
                f"æ¥æº: {result['source']}\n",
                f"ç« èŠ‚: {result['chapter_title']}\n",
                f"è¡ŒèŒƒå›´: {result['line_range']}\n\n",
                f"{result['content']}"
            ]
            return "\n".join(output)

        else:
            return f"âŒ æ— æ•ˆçš„ detail_level: {detail_level}ã€‚è¯·ä½¿ç”¨ 'summary', 'medium', æˆ– 'full'"

    except Exception as e:
        return f"âŒ è·å–ç« èŠ‚å†…å®¹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"


# ==================== å·¥å…· 4ï¼šæ£€ç´¢çŸ¥è¯†åº“ï¼ˆä¼˜åŒ–ï¼‰====================

def search_knowledge(
    query: str,
    top_k: int = 5,
    context_mode: str = "standard"
) -> str:
    """
    æ£€ç´¢çŸ¥è¯†åº“ï¼ˆæ”¯æŒå¤šç§ä¸Šä¸‹æ–‡æ¨¡å¼ï¼‰

    **åŠŸèƒ½ï¼š**
    åŸºäºæŸ¥è¯¢æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼Œæ”¯æŒä¸åŒè¯¦ç»†ç¨‹åº¦çš„ä¸Šä¸‹æ–‡

    **ä½•æ—¶ä½¿ç”¨ï¼š**
    - éœ€è¦æŸ¥æ‰¾ç‰¹å®šä¿¡æ¯æ—¶
    - è·å–ç›¸å…³ç‰‡æ®µçš„ä¸Šä¸‹æ–‡
    - æ¢ç´¢çŸ¥è¯†åº“ä¸­çš„ç›¸å…³å†…å®¹

    **å‚æ•°ï¼š**
    - query (str | required): æŸ¥è¯¢é—®é¢˜æˆ–å…³é”®è¯
    - top_k (int | optional): è¿”å›ç‰‡æ®µæ•°ï¼Œé»˜è®¤ 5ï¼ŒèŒƒå›´ 3-10
    - context_mode (str | optional): ä¸Šä¸‹æ–‡æ¨¡å¼
      - "minimal": ä»…åŒ¹é…ç‰‡æ®µï¼ˆæœ€å°‘ Tokenï¼‰- æœ€å¿«
      - "standard": ç‰‡æ®µ + çŸ­ä¸Šä¸‹æ–‡ï¼ˆ300 å­—ï¼Œé»˜è®¤ï¼‰
      - "expanded": ç‰‡æ®µ + é•¿ä¸Šä¸‹æ–‡ï¼ˆ500 å­—ï¼‰- æœ€è¯¦ç»†

    **è¿”å›ï¼š**
    - åŒ¹é…çš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
    - æ¯ä¸ªç‰‡æ®µåŒ…å«æ¥æºã€ä½ç½®ã€å†…å®¹

    **ç¤ºä¾‹ï¼š**
    - search_knowledge("æ—…æ¸¸å‘å±•ç›®æ ‡")
    - search_knowledge("æŠ•èµ„æ”¿ç­–", top_k=3, context_mode="minimal")
    - search_knowledge("äº§ä¸šå¸ƒå±€", context_mode="expanded")

    **æ³¨æ„ï¼š**
    - "minimal" æ¨¡å¼é€‚åˆå¿«é€ŸæŸ¥æ‰¾
    - "expanded" æ¨¡å¼æä¾›æ›´å¤šä¸Šä¸‹æ–‡ï¼Œä½† Token æ¶ˆè€—æ›´å¤š
    - top_k è¿‡å¤§å¯èƒ½å¯¼è‡´è¿”å›å†…å®¹è¿‡é•¿
    """
    try:
        db = get_vectorstore()

        # æ ¹æ®ä¸Šä¸‹æ–‡æ¨¡å¼è®¾ç½®å‚æ•°
        context_chars_map = {
            "minimal": 0,
            "standard": 300,
            "expanded": 500
        }

        context_chars = context_chars_map.get(context_mode, 300)

        # ä½¿ç”¨æ›´é«˜çš„ k å€¼è·å–æ›´å¤šä¸Šä¸‹æ–‡ï¼ˆé€‚åˆ Planning Agentï¼‰
        results: List[Document] = db.similarity_search(query, k=top_k)

        if not results:
            return "âš ï¸  çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        # æ ¼å¼åŒ–ç»“æœ
        context_parts = []

        for idx, doc in enumerate(results, 1):
            # æå–å…ƒæ•°æ®
            source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
            page = doc.metadata.get("page", doc.metadata.get("paragraph", "æœªçŸ¥"))
            doc_type = doc.metadata.get("type", "æœªçŸ¥ç±»å‹")
            start_index = doc.metadata.get("start_index", 0)

            # æ„å»ºåŸºç¡€ä¸Šä¸‹æ–‡ç‰‡æ®µ
            if context_mode == "minimal":
                # minimal æ¨¡å¼ï¼šä»…è¿”å›æ ¸å¿ƒå†…å®¹
                context_part = [
                    f"ã€ç‰‡æ®µ {idx}ã€‘",
                    f"æ¥æº: {source}",
                    f"ä½ç½®: ç¬¬{page}{doc_type}",
                    f"å†…å®¹: {doc.page_content}"
                ]
            else:
                # standard å’Œ expanded æ¨¡å¼ï¼šåŒ…å«ä¸Šä¸‹æ–‡
                context_part = [
                    f"ã€ç‰‡æ®µ {idx}ã€‘",
                    f"æ¥æº: {source}",
                    f"ä½ç½®: ç¬¬{page}{doc_type}",
                ]

                # å°è¯•è·å–ä¸Šä¸‹æ–‡
                if context_chars > 0 and start_index > 0:
                    try:
                        cm = get_context_manager()
                        ctx = cm.get_context_around_chunk(source, start_index, context_chars)

                        if "error" not in ctx and ctx.get('before') or ctx.get('after'):
                            # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
                            if ctx['before']:
                                context_part.append(f"\nå‰æ–‡:\n{ctx['before'][:200]}...")

                            context_part.append(f"\næ ¸å¿ƒå†…å®¹:\n{doc.page_content}")

                            if ctx['after']:
                                context_part.append(f"\nåæ–‡:\n{ctx['after'][:200]}...")
                        else:
                            # å›é€€åˆ°åŸå§‹æ ¼å¼
                            context_part.append(f"\nå†…å®¹:\n{doc.page_content}")

                    except Exception:
                        # ä¸Šä¸‹æ–‡è·å–å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ ¼å¼
                        context_part.append(f"\nå†…å®¹:\n{doc.page_content}")
                else:
                    # ä¸ä½¿ç”¨ä¸Šä¸‹æ–‡
                    context_part.append(f"\nå†…å®¹:\n{doc.page_content}")

            context_parts.append("\n".join(context_part))

        return "\n\n".join(context_parts)

    except Exception as e:
        return f"âŒ æŸ¥è¯¢çŸ¥è¯†åº“æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"


# ==================== å·¥å…· 5ï¼šæœç´¢å…³é”®è¦ç‚¹ï¼ˆä¿ç•™ï¼Œä¼˜åŒ–æè¿°ï¼‰====================

def search_key_points(query: str, sources: Optional[List[str]] = None) -> str:
    """
    æœç´¢å…³é”®è¦ç‚¹ï¼ˆé¢„å…ˆæå–çš„æ ¸å¿ƒä¿¡æ¯ï¼‰

    **åŠŸèƒ½ï¼š**
    åœ¨æ‰€æœ‰æ–‡æ¡£çš„å…³é”®è¦ç‚¹ä¸­æœç´¢å…³é”®è¯ï¼Œè¦ç‚¹æ˜¯é¢„å…ˆæå–çš„ 10-15 æ¡æ ¸å¿ƒä¿¡æ¯

    **ä½•æ—¶ä½¿ç”¨ï¼š**
    - å¿«é€ŸæŸ¥æ‰¾å…³é”®ä¿¡æ¯ï¼ˆæ¯”å…¨æ–‡æ£€ç´¢æ›´ç²¾ç¡®ï¼‰
    - éœ€è¦"è¦ç‚¹å¼"ç­”æ¡ˆæ—¶
    - æ¢ç´¢æ–‡æ¡£çš„æ ¸å¿ƒè§‚ç‚¹

    **å‚æ•°ï¼š**
    - query (str | required): æœç´¢å…³é”®è¯
    - sources (list[str] | optional): é™åˆ¶æœç´¢çš„æ–‡æ¡£åˆ—è¡¨ï¼Œé»˜è®¤æœç´¢æ‰€æœ‰æ–‡æ¡£

    **è¿”å›ï¼š**
    - åŒ¹é…çš„è¦ç‚¹åˆ—è¡¨
    - æ¯ä¸ªè¦ç‚¹åŒ…å«æ¥æºæ–‡æ¡£å’Œå…·ä½“å†…å®¹

    **ç¤ºä¾‹ï¼š**
    - search_key_points({"query": "æ—…æ¸¸"})
    - search_key_points({"query": "ç›®æ ‡", "sources": ["plan.docx"]})
    - search_key_points({"query": "æŠ•èµ„", "sources": ["plan1.docx", "plan2.docx"]})

    **æ³¨æ„ï¼š**
    - å…³é”®è¦ç‚¹æ˜¯é¢„å…ˆæå–çš„ï¼Œæ¯”å…¨æ–‡æ£€ç´¢æ›´å¿«ã€æ›´ç²¾ç¡®
    - ä»…æœç´¢è¦ç‚¹ï¼Œä¸æœç´¢å®Œæ•´æ–‡æ¡£å†…å®¹
    - å¦‚æœæ²¡æœ‰åŒ¹é…çš„è¦ç‚¹ï¼Œä¼šè¿”å›ç©ºç»“æœ
    """
    try:
        # å…¼å®¹æ—§çš„è°ƒç”¨æ–¹å¼ï¼ˆJSON å­—ç¬¦ä¸²æˆ–å­—å…¸ï¼‰
        if isinstance(query, dict):
            query = query.get("query", "")
            sources = query.get("sources")

        cm = get_context_manager()

        # å¤„ç† sources å‚æ•°
        sources_list = None
        if sources:
            if isinstance(sources, str):
                sources_list = [sources]
            elif isinstance(sources, list):
                sources_list = sources

        result = cm.search_key_points(query, sources_list)

        if result['total_matches'] == 0:
            return f"âš ï¸  æœªæ‰¾åˆ°åŒ…å« '{query}' çš„è¦ç‚¹"

        output = [
            f"ã€å…³é”®è¦ç‚¹æœç´¢ç»“æœã€‘",
            f"æŸ¥è¯¢: {result['query']}",
            f"åŒ¹é…æ•°é‡: {result['total_matches']}\n"
        ]

        for match in result['matches']:
            output.append(
                f"ğŸ“„ {match['source']}\n"
                f"   {match['point']}\n"
            )

        return "\n".join(output)

    except Exception as e:
        return f"âŒ æœç´¢è¦ç‚¹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"


# ==================== å·¥å…· 6ï¼šè·å–å®Œæ•´æ–‡æ¡£ï¼ˆä¿ç•™ï¼‰====================

def get_full_document(source: str) -> str:
    """
    è·å–å®Œæ•´æ–‡æ¡£å†…å®¹

    **åŠŸèƒ½ï¼š**
    è·å–æ–‡æ¡£çš„å®Œæ•´å†…å®¹å’Œå…ƒæ•°æ®

    **ä½•æ—¶ä½¿ç”¨ï¼š**
    - éœ€è¦æ·±åº¦ç†è§£å®Œæ•´è§„åˆ’æ—¶
    - éœ€è¦æŸ¥çœ‹æ–‡æ¡£çš„æ•´ä½“ç»“æ„å’Œå…¨è²Œ
    - éœ€è¦å¼•ç”¨å®Œæ•´æ–‡æ¡£å†…å®¹æ—¶

    **å‚æ•°ï¼š**
    - source (str | required): æ–‡æ¡£åç§°ï¼ˆæ–‡ä»¶åï¼‰

    **è¿”å›ï¼š**
    - å®Œæ•´æ–‡æ¡£å†…å®¹
    - å…ƒæ•°æ®ï¼ˆç±»å‹ã€åˆ‡ç‰‡æ•°ã€å†…å®¹é•¿åº¦ç­‰ï¼‰

    **ç¤ºä¾‹ï¼š**
    - get_full_document("ç½—æµ®-é•¿å®å±±é•‡èåˆå‘å±•æˆ˜ç•¥.pptx")
    - get_full_document("plan.docx")

    **æ³¨æ„ï¼š**
    - æ–‡æ¡£å¯èƒ½å¾ˆé•¿ï¼ˆæ•°ä¸‡å­—ï¼‰ï¼Œä¼šæ¶ˆè€—å¤§é‡ Token
    - è°¨æ…ä½¿ç”¨ï¼Œä¼˜å…ˆè€ƒè™‘ get_document_overview æˆ– get_chapter_content
    """
    try:
        cm = get_context_manager()
        result = cm.get_full_document(source)

        if "error" in result:
            return f"âŒ {result['error']}"

        return (
            f"ã€å®Œæ•´æ–‡æ¡£ã€‘\n"
            f"æ¥æº: {result['source']}\n"
            f"ç±»å‹: {result['doc_type']}\n"
            f"æ€»åˆ‡ç‰‡æ•°: {result['total_chunks']}\n"
            f"å†…å®¹é•¿åº¦: {len(result['content'])} å­—ç¬¦\n\n"
            f"å†…å®¹:\n{result['content']}"
        )

    except Exception as e:
        return f"âŒ è·å–æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"


# ==================== LangChain Tools å®šä¹‰ ====================
# è¿™äº› Tool å¯¹è±¡å¯ä»¥ç›´æ¥é›†æˆåˆ° Agent ä¸­

document_list_tool = Tool(
    name="list_documents",
    func=list_available_documents,
    description=(
        "åˆ—å‡ºçŸ¥è¯†åº“ä¸­æ‰€æœ‰å¯ç”¨çš„æ–‡æ¡£åŠå…¶åŸºæœ¬ä¿¡æ¯ã€‚"
        "åœ¨ä½¿ç”¨å…¶ä»–æ–‡æ¡£å·¥å…·å‰ï¼Œå»ºè®®å…ˆä½¿ç”¨æ­¤å·¥å…·æŸ¥çœ‹æœ‰å“ªäº›æ–‡æ¡£å¯ç”¨ã€‚"
        "\n\n"
        "**ä½•æ—¶ä½¿ç”¨ï¼š**"
        "- ä»»åŠ¡å¼€å§‹æ—¶ï¼Œäº†è§£æœ‰å“ªäº›èµ„æ–™"
        "- ç”¨æˆ·è¯¢é—®'ä½ æœ‰ä»€ä¹ˆçŸ¥è¯†åº“'ã€'ä½ èƒ½åšä»€ä¹ˆ'æ—¶"
        "\n\n"
        "**è¿”å›ï¼š**"
        "- æ–‡æ¡£åç§°ã€ç±»å‹ã€åˆ‡ç‰‡æ•°é‡ã€å†…å®¹é¢„è§ˆ"
    ),
)

document_overview_tool = Tool(
    name="get_document_overview",
    func=lambda params: get_document_overview(**params) if isinstance(params, dict) else get_document_overview(params),
    description=(
        "è·å–æ–‡æ¡£æ¦‚è§ˆï¼ˆæ‰§è¡Œæ‘˜è¦ + å¯é€‰ç« èŠ‚åˆ—è¡¨ï¼‰ã€‚"
        "å¿«é€Ÿäº†è§£æ–‡æ¡£æ ¸å¿ƒå†…å®¹ï¼ŒåŒ…å« 200 å­—æ‰§è¡Œæ‘˜è¦å’Œå¯é€‰çš„ç« èŠ‚åˆ—è¡¨ã€‚"
        "\n\n"
        "**ä½•æ—¶ä½¿ç”¨ï¼š**"
        "- å¿«é€Ÿäº†è§£æ–‡æ¡£æ ¸å¿ƒå†…å®¹"
        "- å†³å®šæ˜¯å¦éœ€è¦æ·±å…¥é˜…è¯»"
        "- å¯¹æ¯”å¤šä¸ªæ–‡æ¡£çš„ä¸»é¢˜"
        "\n\n"
        "**å‚æ•°ï¼ˆJSON æ ¼å¼ï¼‰ï¼š**"
        '- source: æ–‡æ¡£åç§°ï¼ˆæ–‡ä»¶åï¼Œå¿…éœ€ï¼‰'
        '- include_chapters: æ˜¯å¦åŒ…å«ç« èŠ‚åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤ trueï¼‰'
        "\n\n"
        "**ç¤ºä¾‹ï¼š**"
        '- {"source": "ç½—æµ®-é•¿å®å±±é•‡èåˆå‘å±•æˆ˜ç•¥.pptx"}'
        '- {"source": "plan.docx", "include_chapters": false}'
    ),
)

chapter_content_tool = Tool(
    name="get_chapter_content",
    func=lambda params: get_chapter_content(**params) if isinstance(params, dict) else get_chapter_content(params, ""),
    description=(
        "è·å–ç« èŠ‚å†…å®¹ï¼ˆæ”¯æŒä¸‰çº§è¯¦æƒ…ï¼‰ã€‚"
        "æ ¹æ®éœ€æ±‚è·å–ä¸åŒè¯¦ç»†ç¨‹åº¦çš„ç« èŠ‚å†…å®¹ï¼Œä»æ‘˜è¦åˆ°å®Œæ•´å†…å®¹ã€‚"
        "\n\n"
        "**ä½•æ—¶ä½¿ç”¨ï¼š**"
        "- äº†è§£ç‰¹å®šç« èŠ‚å†…å®¹æ—¶"
        "- æ ¹æ®ä¿¡æ¯éœ€æ±‚é€‰æ‹©åˆé€‚çš„è¯¦ç»†ç¨‹åº¦"
        "- å¿«é€Ÿæµè§ˆæˆ–æ·±åº¦é˜…è¯»ç‰¹å®šç« èŠ‚"
        "\n\n"
        "**å‚æ•°ï¼ˆJSON æ ¼å¼ï¼‰ï¼š**"
        '- source: æ–‡æ¡£åç§°ï¼ˆæ–‡ä»¶åï¼Œå¿…éœ€ï¼‰'
        '- chapter_pattern: ç« èŠ‚æ ‡é¢˜å…³é”®è¯ï¼ˆå¿…éœ€ï¼Œæ”¯æŒéƒ¨åˆ†åŒ¹é…ï¼‰'
        '- detail_level: è¯¦ç»†ç¨‹åº¦ï¼ˆå¯é€‰ï¼Œé»˜è®¤ "medium"ï¼‰'
        '  * "summary": ä»…æ‘˜è¦ï¼ˆ100-200 å­—ï¼‰- æœ€å¿«'
        '  * "medium": æ‘˜è¦ + å…³é”®è¦ç‚¹ï¼ˆé»˜è®¤ï¼‰'
        '  * "full": å®Œæ•´ç« èŠ‚å†…å®¹ - æœ€è¯¦ç»†'
        "\n\n"
        "**ç¤ºä¾‹ï¼š**"
        '- å¿«é€Ÿæµè§ˆ: {"source": "plan.docx", "chapter_pattern": "ç¬¬ä¸€ç« ", "detail_level": "summary"}'
        '- ä¸­ç­‰æ·±åº¦: {"source": "plan.docx", "chapter_pattern": "äº§ä¸š", "detail_level": "medium"}'
        '- æ·±åº¦é˜…è¯»: {"source": "plan.docx", "chapter_pattern": "æŠ•èµ„", "detail_level": "full"}'
    ),
)

knowledge_search_tool = Tool(
    name="search_knowledge",
    func=lambda params: search_knowledge(**params) if isinstance(params, dict) else search_knowledge(params),
    description=(
        "æ£€ç´¢çŸ¥è¯†åº“ï¼ˆæ”¯æŒå¤šç§ä¸Šä¸‹æ–‡æ¨¡å¼ï¼‰ã€‚"
        "åŸºäºæŸ¥è¯¢æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼Œæ”¯æŒä¸åŒè¯¦ç»†ç¨‹åº¦çš„ä¸Šä¸‹æ–‡ã€‚"
        "\n\n"
        "**ä½•æ—¶ä½¿ç”¨ï¼š**"
        "- éœ€è¦æŸ¥æ‰¾ç‰¹å®šä¿¡æ¯æ—¶"
        "- è·å–ç›¸å…³ç‰‡æ®µçš„ä¸Šä¸‹æ–‡"
        "- æ¢ç´¢çŸ¥è¯†åº“ä¸­çš„ç›¸å…³å†…å®¹"
        "\n\n"
        "**å‚æ•°ï¼ˆJSON æ ¼å¼ï¼‰ï¼š**"
        '- query: æŸ¥è¯¢é—®é¢˜æˆ–å…³é”®è¯ï¼ˆå¿…éœ€ï¼‰'
        '- top_k: è¿”å›ç‰‡æ®µæ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ 5ï¼ŒèŒƒå›´ 3-10ï¼‰'
        '- context_mode: ä¸Šä¸‹æ–‡æ¨¡å¼ï¼ˆå¯é€‰ï¼Œé»˜è®¤ "standard"ï¼‰'
        '  * "minimal": ä»…åŒ¹é…ç‰‡æ®µï¼ˆæœ€å°‘ Tokenï¼‰- æœ€å¿«'
        '  * "standard": ç‰‡æ®µ + çŸ­ä¸Šä¸‹æ–‡ï¼ˆ300 å­—ï¼Œé»˜è®¤ï¼‰'
        '  * "expanded": ç‰‡æ®µ + é•¿ä¸Šä¸‹æ–‡ï¼ˆ500 å­—ï¼‰- æœ€è¯¦ç»†'
        "\n\n"
        "**ç¤ºä¾‹ï¼š**"
        '- search_knowledge({"query": "æ—…æ¸¸å‘å±•ç›®æ ‡"})'
        '- search_knowledge({"query": "æŠ•èµ„æ”¿ç­–", "top_k": 3, "context_mode": "minimal"})'
        '- search_knowledge({"query": "äº§ä¸šå¸ƒå±€", "context_mode": "expanded"})'
    ),
)

key_points_search_tool = Tool(
    name="search_key_points",
    func=search_key_points,
    description=(
        "æœç´¢å…³é”®è¦ç‚¹ï¼ˆé¢„å…ˆæå–çš„æ ¸å¿ƒä¿¡æ¯ï¼‰ã€‚"
        "åœ¨æ‰€æœ‰æ–‡æ¡£çš„å…³é”®è¦ç‚¹ä¸­æœç´¢å…³é”®è¯ï¼Œè¦ç‚¹æ˜¯é¢„å…ˆæå–çš„ 10-15 æ¡æ ¸å¿ƒä¿¡æ¯ã€‚"
        "\n\n"
        "**ä½•æ—¶ä½¿ç”¨ï¼š**"
        "- å¿«é€ŸæŸ¥æ‰¾å…³é”®ä¿¡æ¯ï¼ˆæ¯”å…¨æ–‡æ£€ç´¢æ›´ç²¾ç¡®ï¼‰"
        "- éœ€è¦'è¦ç‚¹å¼'ç­”æ¡ˆæ—¶"
        "- æ¢ç´¢æ–‡æ¡£çš„æ ¸å¿ƒè§‚ç‚¹"
        "\n\n"
        "**å‚æ•°ï¼ˆJSON æ ¼å¼ï¼‰ï¼š**"
        '- query: æœç´¢å…³é”®è¯ï¼ˆå¿…éœ€ï¼‰'
        '- sources: é™åˆ¶æœç´¢çš„æ–‡æ¡£åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰'
        "\n\n"
        "**ç¤ºä¾‹ï¼š**"
        '- {"query": "æ—…æ¸¸"}'
        '- {"query": "ç›®æ ‡", "sources": "plan.docx"}'
        '- {"query": "æŠ•èµ„", "sources": ["plan1.docx", "plan2.docx"]}'
        "\n\n"
        "**æ³¨æ„ï¼š**"
        "å…³é”®è¦ç‚¹æ˜¯é¢„å…ˆæå–çš„ï¼Œæ¯”å…¨æ–‡æ£€ç´¢æ›´å¿«ã€æ›´ç²¾ç¡®ã€‚"
    ),
)

full_document_tool = Tool(
    name="get_document_full",
    func=get_full_document,
    description=(
        "è·å–å®Œæ•´æ–‡æ¡£å†…å®¹ã€‚"
        "è·å–æ–‡æ¡£çš„å®Œæ•´å†…å®¹å’Œå…ƒæ•°æ®ã€‚"
        "\n\n"
        "**ä½•æ—¶ä½¿ç”¨ï¼š**"
        "- éœ€è¦æ·±åº¦ç†è§£å®Œæ•´è§„åˆ’æ—¶"
        "- éœ€è¦æŸ¥çœ‹æ–‡æ¡£çš„æ•´ä½“ç»“æ„å’Œå…¨è²Œ"
        "- éœ€è¦å¼•ç”¨å®Œæ•´æ–‡æ¡£å†…å®¹æ—¶"
        "\n\n"
        "**å‚æ•°ï¼š**"
        '- source: æ–‡æ¡£åç§°ï¼ˆæ–‡ä»¶åï¼Œå¿…éœ€ï¼‰'
        "\n\n"
        "**ç¤ºä¾‹ï¼š**"
        '- {"source": "ç½—æµ®-é•¿å®å±±é•‡èåˆå‘å±•æˆ˜ç•¥.pptx"}'
        '- {"source": "plan.docx"}'
        "\n\n"
        "**æ³¨æ„ï¼š**"
        "æ–‡æ¡£å¯èƒ½å¾ˆé•¿ï¼ˆæ•°ä¸‡å­—ï¼‰ï¼Œä¼šæ¶ˆè€—å¤§é‡ Tokenã€‚"
        "è°¨æ…ä½¿ç”¨ï¼Œä¼˜å…ˆè€ƒè™‘ get_document_overview æˆ– get_chapter_contentã€‚"
    ),
)


# ==================== å¯¼å‡ºå·¥å…·åˆ—è¡¨ ====================

# æ ¸å¿ƒå·¥å…·é›†ï¼ˆ7 ä¸ªå·¥å…·ï¼‰
PLANNING_TOOLS = [
    # åŸºç¡€å·¥å…·
    document_list_tool,

    # å¿«é€Ÿæ¨¡å¼å·¥å…·
    document_overview_tool,
    key_points_search_tool,

    # æ·±åº¦æ¨¡å¼å·¥å…·
    knowledge_search_tool,
    chapter_content_tool,
    full_document_tool,
]

# å‘åå…¼å®¹ï¼šä¿ç•™æ—§çš„å·¥å…·åç§°
planning_knowledge_tool = knowledge_search_tool  # åˆ«å
executive_summary_tool = document_overview_tool  # åˆ«å
chapter_summaries_list_tool = document_overview_tool  # åˆ«å
chapter_summary_tool = chapter_content_tool  # åˆ«å
chapter_context_tool = chapter_content_tool  # åˆ«å
context_around_tool = knowledge_search_tool  # åˆ«å


# ==================== æ—§ç‰ˆå·¥å…·ï¼ˆå…¼å®¹æ€§ï¼‰====================

# ä¿ç•™æ—§çš„ retrieve_planning_knowledge å‡½æ•°ä»¥å…¼å®¹ç°æœ‰ä»£ç 
def retrieve_planning_knowledge(
    query: str,
    top_k: int = DEFAULT_TOP_K,
    with_context: bool = True,
    context_chars: int = 300
) -> str:
    """
    æ£€ç´¢ä¹¡æ‘è§„åˆ’ç›¸å…³çŸ¥è¯†ï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰

    **æ³¨æ„ï¼š** è¿™æ˜¯ä¸€ä¸ªå…¼å®¹æ€§å‡½æ•°ï¼Œå»ºè®®ä½¿ç”¨æ–°çš„ search_knowledge å·¥å…·
    """
    # æ ¹æ®æ—§å‚æ•°æ˜ å°„åˆ°æ–°çš„ context_mode
    if not with_context or context_chars == 0:
        context_mode = "minimal"
    elif context_chars >= 500:
        context_mode = "expanded"
    else:
        context_mode = "standard"

    return search_knowledge(query, top_k, context_mode)


# æ—§çš„ Agentic RAG æ¨¡å¼å·¥å…·ï¼ˆå…¼å®¹æ€§ï¼‰
from langchain_core.tools import tool

@tool(response_format="content_and_artifact")
def retrieve_knowledge_detailed(query: str) -> tuple[str, List[Document]]:
    """
    æ£€ç´¢çŸ¥è¯†ï¼ˆAgentic RAG æ¨¡å¼ï¼Œå…¼å®¹æ—§ç‰ˆï¼‰
    è¿”å›æ ¼å¼åŒ–æ–‡æœ¬ + åŸå§‹æ–‡æ¡£å¯¹è±¡

    **æ³¨æ„ï¼š** è¿™æ˜¯ä¸€ä¸ªå…¼å®¹æ€§å‡½æ•°ï¼Œå»ºè®®ä½¿ç”¨æ–°çš„ search_knowledge å·¥å…·
    """
    db = get_vectorstore()
    retrieved_docs = db.similarity_search(query, k=DEFAULT_TOP_K)

    # æ ¼å¼åŒ–å†…å®¹
    serialized = "\n\n".join(
        f"æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}\n"
        f"ä½ç½®: {doc.metadata.get('page', doc.metadata.get('paragraph', 'æœªçŸ¥'))}\n"
        f"å†…å®¹: {doc.page_content}"
        for doc in retrieved_docs
    )

    return serialized, retrieved_docs
