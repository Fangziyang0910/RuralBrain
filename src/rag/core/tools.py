"""
çŸ¥è¯†åº“æ£€ç´¢å·¥å…·ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

åŸºäº references/agent_skills æœ€ä½³å®è·µé‡æ„ï¼š
1. åº”ç”¨ Consolidation Principle - 7 ä¸ªæ ¸å¿ƒå·¥å…·
2. ä¼˜åŒ–å·¥å…·æè¿° - éµå¾ª"åšä»€ä¹ˆã€ä½•æ—¶ç”¨ã€è¿”å›ä»€ä¹ˆ"åŸåˆ™
3. ç»Ÿä¸€å‚æ•°æ ¼å¼ - ä½¿ç”¨ä¸€è‡´çš„è®¾è®¡æ¨¡å¼
4. æ”¯æŒæ¸è¿›å¼æŠ«éœ² - é€šè¿‡å‚æ•°æ§åˆ¶è¿”å›è¯¦ç»†ç¨‹åº¦
"""
from pathlib import Path
from typing import Optional

from langchain_core.documents import Document
from langchain_core.tools import Tool, tool

import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.rag.config import DEFAULT_TOP_K
from src.rag.core.context_manager import get_context_manager
from src.rag.core.cache import get_vector_cache


# ==================== è¾…åŠ©å‡½æ•° ====================

def get_vectorstore():
    """è·å–å‘é‡æ•°æ®åº“ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰"""
    return get_vector_cache().get_vectorstore()


def format_error(message: str, error: Exception) -> str:
    """æ ¼å¼åŒ–é”™è¯¯æ¶ˆæ¯"""
    return f"âŒ {message}æ—¶å‘ç”Ÿé”™è¯¯: {error}"


# ==================== å·¥å…·å‡½æ•° ====================

def list_available_documents(query: str = "") -> str:
    """
    åˆ—å‡ºçŸ¥è¯†åº“ä¸­æ‰€æœ‰å¯ç”¨æ–‡æ¡£

    **ä½•æ—¶ä½¿ç”¨ï¼š**
    - ä»»åŠ¡å¼€å§‹æ—¶ï¼Œäº†è§£æœ‰å“ªäº›èµ„æ–™
    - ç”¨æˆ·è¯¢é—®"ä½ æœ‰ä»€ä¹ˆçŸ¥è¯†åº“"ã€"ä½ èƒ½åšä»€ä¹ˆ"æ—¶

    **è¿”å›ï¼š**
    - æ–‡æ¡£åç§°ã€ç±»å‹ã€åˆ‡ç‰‡æ•°é‡ã€å†…å®¹é¢„è§ˆ
    """
    try:
        cm = get_context_manager()
        cm._ensure_loaded()

        if not cm.doc_index:
            return "âš ï¸  çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡æ¡£"

        lines = ["ã€å¯ç”¨æ–‡æ¡£åˆ—è¡¨ã€‘\n"]

        for idx, (source, doc_idx) in enumerate(cm.doc_index.items(), 1):
            preview = doc_idx.chunks_info[0]['content_preview'] if doc_idx.chunks_info else 'N/A'
            lines.append(
                f"{idx}. {source}\n"
                f"   ç±»å‹: {doc_idx.doc_type}\n"
                f"   åˆ‡ç‰‡æ•°: {len(doc_idx.chunks_info)}\n"
                f"   é¢„è§ˆ: {preview}\n"
            )

        return "\n".join(lines)

    except Exception as e:
        return format_error("åˆ—å‡ºæ–‡æ¡£", e)


def get_document_overview(source: str, include_chapters: bool = True) -> str:
    """
    è·å–æ–‡æ¡£æ¦‚è§ˆï¼ˆæ‰§è¡Œæ‘˜è¦ + å¯é€‰ç« èŠ‚åˆ—è¡¨ï¼‰

    **ä½•æ—¶ä½¿ç”¨ï¼š**
    - å¿«é€Ÿäº†è§£æ–‡æ¡£æ ¸å¿ƒå†…å®¹
    - å†³å®šæ˜¯å¦éœ€è¦æ·±å…¥é˜…è¯»
    - å¯¹æ¯”å¤šä¸ªæ–‡æ¡£çš„ä¸»é¢˜

    **å‚æ•°ï¼š**
    - source (str | required): æ–‡æ¡£åç§°ï¼ˆæ–‡ä»¶åï¼‰
    - include_chapters (bool | optional): æ˜¯å¦åŒ…å«ç« èŠ‚åˆ—è¡¨ï¼Œé»˜è®¤ True

    **è¿”å›ï¼š**
    - æ‰§è¡Œæ‘˜è¦ï¼ˆ200 å­—ï¼‰
    - ç« èŠ‚æ ‡é¢˜åˆ—è¡¨ï¼ˆå¦‚æœ include_chapters=Trueï¼‰
    """
    try:
        cm = get_context_manager()
        result = cm.get_executive_summary(source)

        if "error" in result:
            return f"âŒ {result['error']}"

        lines = [
            f"ã€æ–‡æ¡£æ¦‚è§ˆã€‘\n",
            f"æ¥æº: {result['source']}\n",
            f"ç±»å‹: {result.get('doc_type', 'æœªçŸ¥')}\n\n",
        ]

        if result.get("executive_summary"):
            lines.append(f"**æ‰§è¡Œæ‘˜è¦**\n{result['executive_summary']}\n")
        else:
            lines.append(f"âš ï¸  è¯¥æ–‡æ¡£å°šæœªç”Ÿæˆæ‘˜è¦\n")

        if include_chapters:
            chapters_result = cm.list_chapter_summaries(source)
            if chapters_result.get("chapters"):
                lines.append(f"\n**ç« èŠ‚åˆ—è¡¨**\n")
                lines.extend(f"{idx}. {chapter['title']}\n" for idx, chapter in enumerate(chapters_result['chapters'], 1))

        return "\n".join(lines)

    except Exception as e:
        return format_error("è·å–æ–‡æ¡£æ¦‚è§ˆ", e)


def get_chapter_content(source: str, chapter_pattern: str, detail_level: str = "medium") -> str:
    """
    è·å–ç« èŠ‚å†…å®¹ï¼ˆæ”¯æŒä¸‰çº§è¯¦æƒ…ï¼‰

    **ä½•æ—¶ä½¿ç”¨ï¼š**
    - äº†è§£ç‰¹å®šç« èŠ‚å†…å®¹æ—¶
    - æ ¹æ®ä¿¡æ¯éœ€æ±‚é€‰æ‹©åˆé€‚çš„è¯¦ç»†ç¨‹åº¦
    - å¿«é€Ÿæµè§ˆæˆ–æ·±åº¦é˜…è¯»ç‰¹å®šç« èŠ‚

    **å‚æ•°ï¼š**
    - source (str | required): æ–‡æ¡£åç§°ï¼ˆæ–‡ä»¶åï¼‰
    - chapter_pattern (str | required): ç« èŠ‚æ ‡é¢˜å…³é”®è¯ï¼ˆæ”¯æŒéƒ¨åˆ†åŒ¹é…ï¼‰
    - detail_level (str | optional): è¯¦ç»†ç¨‹åº¦
      - "summary": ä»…æ‘˜è¦ï¼ˆ100-200 å­—ï¼‰- æœ€å¿«
      - "medium": æ‘˜è¦ + å…³é”®è¦ç‚¹ï¼ˆé»˜è®¤ï¼‰
      - "full": å®Œæ•´ç« èŠ‚å†…å®¹ - æœ€è¯¦ç»†

    **è¿”å›ï¼š**
    - æ ¹æ® detail_level è¿”å›ä¸åŒè¯¦ç»†ç¨‹åº¦çš„ç« èŠ‚å†…å®¹
    """
    try:
        cm = get_context_manager()

        if detail_level == "summary":
            result = cm.get_chapter_summary(source, chapter_pattern)
            if "error" in result:
                return f"âŒ {result['error']}"
            return f"ã€çŸ¥è¯†ç‰‡æ®µ 1ã€‘\næ¥æº: {result['source']}\nä½ç½®: ç¬¬1 é¡µ\nå†…å®¹:\n{result['summary']}"

        elif detail_level == "medium":
            result = cm.get_chapter_summary(source, chapter_pattern)
            if "error" in result:
                return f"âŒ {result['error']}"

            lines = [
                f"ã€çŸ¥è¯†ç‰‡æ®µ 1ã€‘\n",
                f"æ¥æº: {result['source']}\n",
                f"ä½ç½®: ç¬¬1 é¡µ\n",
                f"å†…å®¹:\n**æ‘˜è¦**\n{result['summary']}\n\n",
                f"**å…³é”®è¦ç‚¹**\n"
            ]
            lines.extend(f"  â€¢ {point}" for point in result.get('key_points', []))
            return "\n".join(lines)

        elif detail_level == "full":
            result = cm.get_chapter_by_header(source, chapter_pattern)
            if "error" in result:
                return f"âŒ {result['error']}"
            return f"ã€çŸ¥è¯†ç‰‡æ®µ 1ã€‘\næ¥æº: {result['source']}\nä½ç½®: ç¬¬1 é¡µ\nå†…å®¹:\n{result['content']}"

        else:
            return f"âŒ æ— æ•ˆçš„ detail_level: {detail_level}ã€‚è¯·ä½¿ç”¨ 'summary', 'medium', æˆ– 'full'"

    except Exception as e:
        return format_error("è·å–ç« èŠ‚å†…å®¹", e)


def search_knowledge(query: str, top_k: int = 5, context_mode: str = "standard") -> str:
    """
    æ£€ç´¢çŸ¥è¯†åº“ï¼ˆæ”¯æŒå¤šç§ä¸Šä¸‹æ–‡æ¨¡å¼ï¼‰

    **ä½•æ—¶ä½¿ç”¨ï¼š**
    - éœ€è¦æŸ¥æ‰¾ç‰¹å®šä¿¡æ¯æ—¶
    - è·å–ç›¸å…³ç‰‡æ®µçš„ä¸Šä¸‹æ–‡
    - æ¢ç´¢çŸ¥è¯†åº“ä¸­çš„ç›¸å…³å†…å®¹

    **å‚æ•°ï¼š**
    - query (str | required): æŸ¥è¯¢é—®é¢˜æˆ–å…³é”®è¯
    - top_k (int | optional): è¿”å›ç‰‡æ®µæ•°ï¼Œé»˜è®¤ 5ï¼ŒèŒƒå›´ 3-10
    - context_mode (str | optional): ä¸Šä¸‹æ–‡æ¨¡å¼
      - "minimal": ä»…åŒ¹é…ç‰‡æ®µï¼ˆæœ€å°‘ Tokenï¼‰- æœ€å¿«
      - "standard": ç‰‡æ®µ + çŸ­ä¸Šä¸‹æ–‡ï¼ˆ300 å­—ï¼Œé»˜è®¤ï¼‰
      - "expanded": ç‰‡æ®µ + é•¿ä¸Šä¸‹æ–‡ï¼ˆ500 å­—ï¼‰- æœ€è¯¦ç»†

    **è¿”å›ï¼š**
    - åŒ¹é…çš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨ï¼ŒåŒ…å«æ¥æºã€ä½ç½®ã€å†…å®¹
    """
    try:
        db = get_vectorstore()
        context_chars_map = {"minimal": 0, "standard": 300, "expanded": 500}
        context_chars = context_chars_map.get(context_mode, 300)

        results: list[Document] = db.similarity_search(query, k=top_k)

        if not results:
            return "âš ï¸  çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        fragments = []

        for idx, doc in enumerate(results, 1):
            source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
            page = doc.metadata.get("page", doc.metadata.get("paragraph", "æœªçŸ¥"))
            doc_type = doc.metadata.get("type", "æœªçŸ¥ç±»å‹")
            start_index = doc.metadata.get("start_index", 0)

            fragment = [f"ã€çŸ¥è¯†ç‰‡æ®µ {idx}ã€‘", f"æ¥æº: {source}", f"ä½ç½®: ç¬¬{page} {doc_type}"]

            if context_mode == "minimal":
                fragment.append(f"å†…å®¹: {doc.page_content}")
            elif context_chars > 0 and start_index > 0:
                try:
                    cm = get_context_manager()
                    ctx = cm.get_context_around_chunk(source, start_index, context_chars)

                    if "error" not in ctx and (ctx.get('before') or ctx.get('after')):
                        if ctx['before']:
                            fragment.append(f"\nå‰æ–‡:\n{ctx['before'][:200]}...")
                        fragment.append(f"\næ ¸å¿ƒå†…å®¹:\n{doc.page_content}")
                        if ctx['after']:
                            fragment.append(f"\nåæ–‡:\n{ctx['after'][:200]}...")
                    else:
                        fragment.append(f"\nå†…å®¹:\n{doc.page_content}")
                except Exception:
                    fragment.append(f"\nå†…å®¹:\n{doc.page_content}")
            else:
                fragment.append(f"\nå†…å®¹:\n{doc.page_content}")

            fragments.append("\n".join(fragment))

        return "\n\n".join(fragments)

    except Exception as e:
        return format_error("æŸ¥è¯¢çŸ¥è¯†åº“", e)


def search_key_points(query: str, sources: Optional[list[str]] = None) -> str:
    """
    æœç´¢å…³é”®è¦ç‚¹ï¼ˆé¢„å…ˆæå–çš„æ ¸å¿ƒä¿¡æ¯ï¼‰

    **ä½•æ—¶ä½¿ç”¨ï¼š**
    - å¿«é€ŸæŸ¥æ‰¾å…³é”®ä¿¡æ¯ï¼ˆæ¯”å…¨æ–‡æ£€ç´¢æ›´ç²¾ç¡®ï¼‰
    - éœ€è¦"è¦ç‚¹å¼"ç­”æ¡ˆæ—¶
    - æ¢ç´¢æ–‡æ¡£çš„æ ¸å¿ƒè§‚ç‚¹

    **å‚æ•°ï¼š**
    - query (str | required): æœç´¢å…³é”®è¯
    - sources (list[str] | optional): é™åˆ¶æœç´¢çš„æ–‡æ¡£åˆ—è¡¨ï¼Œé»˜è®¤æœç´¢æ‰€æœ‰æ–‡æ¡£

    **è¿”å›ï¼š**
    - åŒ¹é…çš„è¦ç‚¹åˆ—è¡¨ï¼ŒåŒ…å«æ¥æºæ–‡æ¡£å’Œå…·ä½“å†…å®¹
    """
    try:
        # å…¼å®¹æ—§çš„è°ƒç”¨æ–¹å¼
        if isinstance(query, dict):
            query = query.get("query", "")
            sources = query.get("sources")

        cm = get_context_manager()

        sources_list = None
        if sources:
            sources_list = [sources] if isinstance(sources, str) else sources

        result = cm.search_key_points(query, sources_list)

        if result['total_matches'] == 0:
            return f"âš ï¸  æœªæ‰¾åˆ°åŒ…å« '{query}' çš„è¦ç‚¹"

        lines = [
            f"ã€å…³é”®è¦ç‚¹æœç´¢ç»“æœã€‘",
            f"æŸ¥è¯¢: {result['query']}",
            f"åŒ¹é…æ•°é‡: {result['total_matches']}\n"
        ]

        for match in result['matches']:
            lines.append(f"ğŸ“„ {match['source']}\n   {match['point']}\n")

        return "\n".join(lines)

    except Exception as e:
        return format_error("æœç´¢è¦ç‚¹", e)


def get_full_document(source: str) -> str:
    """
    è·å–å®Œæ•´æ–‡æ¡£å†…å®¹

    **ä½•æ—¶ä½¿ç”¨ï¼š**
    - éœ€è¦æ·±åº¦ç†è§£å®Œæ•´è§„åˆ’æ—¶
    - éœ€è¦æŸ¥çœ‹æ–‡æ¡£çš„æ•´ä½“ç»“æ„å’Œå…¨è²Œ
    - éœ€è¦å¼•ç”¨å®Œæ•´æ–‡æ¡£å†…å®¹æ—¶

    **å‚æ•°ï¼š**
    - source (str | required): æ–‡æ¡£åç§°ï¼ˆæ–‡ä»¶åï¼‰

    **è¿”å›ï¼š**
    - å®Œæ•´æ–‡æ¡£å†…å®¹å’Œå…ƒæ•°æ®ï¼ˆç±»å‹ã€åˆ‡ç‰‡æ•°ã€å†…å®¹é•¿åº¦ç­‰ï¼‰

    **æ³¨æ„ï¼š**
    - æ–‡æ¡£å¯èƒ½å¾ˆé•¿ï¼ˆæ•°ä¸‡å­—ï¼‰ï¼Œä¼šæ¶ˆè€—å¤§é‡ Token
    - è°¨æ…ä½¿ç”¨ï¼Œä¼˜å…ˆè€ƒè™‘ get_document_overview æˆ– get_chapter_content
    """
    try:
        cm = get_context_manager()
        result = cm.get_full_document(source)

        if "error" in result:
            return f"âŒ {result['error']}"

        return (
            f"ã€å®Œæ•´æ–‡æ¡£ã€‘\n"
            f"æ¥æº: {result['source']}\n"
            f"ç±»å‹: {result['doc_type']}\n"
            f"æ€»åˆ‡ç‰‡æ•°: {result['total_chunks']}\n"
            f"å†…å®¹é•¿åº¦: {len(result['content'])} å­—ç¬¦\n\n"
            f"å†…å®¹:\n{result['content']}"
        )

    except Exception as e:
        return format_error("è·å–æ–‡æ¡£", e)


# ==================== LangChain Tools å®šä¹‰ ====================

def create_tool(name: str, func, description_template: str) -> Tool:
    """åˆ›å»ºå·¥å…·çš„è¾…åŠ©å‡½æ•°"""
    return Tool(name=name, func=func, description=description_template)


document_list_tool = Tool(
    name="list_documents",
    func=list_available_documents,
    description="åˆ—å‡ºçŸ¥è¯†åº“ä¸­æ‰€æœ‰å¯ç”¨çš„æ–‡æ¡£åŠå…¶åŸºæœ¬ä¿¡æ¯ã€‚åœ¨ä½¿ç”¨å…¶ä»–æ–‡æ¡£å·¥å…·å‰ï¼Œå»ºè®®å…ˆä½¿ç”¨æ­¤å·¥å…·æŸ¥çœ‹æœ‰å“ªäº›æ–‡æ¡£å¯ç”¨ã€‚",
)

# ä½¿ç”¨ @tool è£…é¥°å™¨é‡æ–°å®šä¹‰å·¥å…·ï¼ˆä¿®å¤å‚æ•°ä¼ é€’é—®é¢˜ï¼‰
@tool
def document_overview_tool(source: str, include_chapters: bool = True) -> str:
    """
    è·å–æ–‡æ¡£æ¦‚è§ˆï¼ˆæ‰§è¡Œæ‘˜è¦ + å¯é€‰ç« èŠ‚åˆ—è¡¨ï¼‰ã€‚

    å¿«é€Ÿäº†è§£æ–‡æ¡£æ ¸å¿ƒå†…å®¹ï¼ŒåŒ…å« 200 å­—æ‰§è¡Œæ‘˜è¦å’Œå¯é€‰çš„ç« èŠ‚åˆ—è¡¨ã€‚

    Args:
        source: æ–‡æ¡£åç§°ï¼ˆæ–‡ä»¶åï¼Œå¿…éœ€ï¼‰
        include_chapters: æ˜¯å¦åŒ…å«ç« èŠ‚åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤ trueï¼‰

    Returns:
        æ‰§è¡Œæ‘˜è¦å’Œå¯é€‰çš„ç« èŠ‚åˆ—è¡¨
    """
    return get_document_overview(source, include_chapters)

@tool
def chapter_content_tool(source: str, chapter_pattern: str, detail_level: str = "medium") -> str:
    """
    è·å–ç« èŠ‚å†…å®¹ï¼ˆæ”¯æŒä¸‰çº§è¯¦æƒ…ï¼‰ã€‚

    æ ¹æ®éœ€æ±‚è·å–ä¸åŒè¯¦ç»†ç¨‹åº¦çš„ç« èŠ‚å†…å®¹ï¼Œä»æ‘˜è¦åˆ°å®Œæ•´å†…å®¹ã€‚

    Args:
        source: æ–‡æ¡£åç§°ï¼ˆæ–‡ä»¶åï¼Œå¿…éœ€ï¼‰
        chapter_pattern: ç« èŠ‚æ ‡é¢˜å…³é”®è¯ï¼ˆå¿…éœ€ï¼Œæ”¯æŒéƒ¨åˆ†åŒ¹é…ï¼‰
        detail_level: è¯¦ç»†ç¨‹åº¦ï¼ˆå¯é€‰ï¼Œé»˜è®¤ "medium"ï¼‰
            - "summary": ä»…æ‘˜è¦ï¼ˆ100-200 å­—ï¼‰
            - "medium": æ‘˜è¦ + å…³é”®è¦ç‚¹ï¼ˆé»˜è®¤ï¼‰
            - "full": å®Œæ•´ç« èŠ‚å†…å®¹

    Returns:
        æ ¹æ® detail_level è¿”å›ä¸åŒè¯¦ç»†ç¨‹åº¦çš„ç« èŠ‚å†…å®¹
    """
    return get_chapter_content(source, chapter_pattern, detail_level)

@tool
def knowledge_search_tool(query: str, top_k: int = 5, context_mode: str = "standard") -> str:
    """
    æ£€ç´¢çŸ¥è¯†åº“ï¼ˆæ”¯æŒå¤šç§ä¸Šä¸‹æ–‡æ¨¡å¼ï¼‰ã€‚

    åŸºäºæŸ¥è¯¢æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼Œæ”¯æŒä¸åŒè¯¦ç»†ç¨‹åº¦çš„ä¸Šä¸‹æ–‡ã€‚

    Args:
        query: æŸ¥è¯¢é—®é¢˜æˆ–å…³é”®è¯ï¼ˆå¿…éœ€ï¼‰
        top_k: è¿”å›ç‰‡æ®µæ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ 5ï¼ŒèŒƒå›´ 3-10ï¼‰
        context_mode: ä¸Šä¸‹æ–‡æ¨¡å¼ï¼ˆå¯é€‰ï¼Œé»˜è®¤ "standard"ï¼‰
            - "minimal": ä»…åŒ¹é…ç‰‡æ®µï¼ˆæœ€å°‘ Tokenï¼‰
            - "standard": ç‰‡æ®µ + çŸ­ä¸Šä¸‹æ–‡ï¼ˆ300 å­—ï¼Œé»˜è®¤ï¼‰
            - "expanded": ç‰‡æ®µ + é•¿ä¸Šä¸‹æ–‡ï¼ˆ500 å­—ï¼‰

    Returns:
        åŒ¹é…çš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨ï¼ŒåŒ…å«æ¥æºã€ä½ç½®ã€å†…å®¹
    """
    return search_knowledge(query, top_k, context_mode)

key_points_search_tool = Tool(
    name="search_key_points",
    func=search_key_points,
    description=(
        "æœç´¢å…³é”®è¦ç‚¹ï¼ˆé¢„å…ˆæå–çš„æ ¸å¿ƒä¿¡æ¯ï¼‰ã€‚åœ¨æ‰€æœ‰æ–‡æ¡£çš„å…³é”®è¦ç‚¹ä¸­æœç´¢å…³é”®è¯ã€‚\n\n"
        "**å‚æ•°ï¼ˆJSON æ ¼å¼ï¼‰ï¼š**\n"
        '- query: æœç´¢å…³é”®è¯ï¼ˆå¿…éœ€ï¼‰\n'
        '- sources: é™åˆ¶æœç´¢çš„æ–‡æ¡£åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰\n\n'
        "**ç¤ºä¾‹ï¼š**\n"
        '- {"query": "æ—…æ¸¸"}\n'
        '- {"query": "ç›®æ ‡", "sources": "plan.docx"}\n'
        '- {"query": "æŠ•èµ„", "sources": ["plan1.docx", "plan2.docx"]}'
    ),
)

full_document_tool = Tool(
    name="get_document_full",
    func=get_full_document,
    description=(
        "è·å–å®Œæ•´æ–‡æ¡£å†…å®¹ã€‚è·å–æ–‡æ¡£çš„å®Œæ•´å†…å®¹å’Œå…ƒæ•°æ®ã€‚\n\n"
        "**å‚æ•°ï¼š**\n"
        '- source: æ–‡æ¡£åç§°ï¼ˆæ–‡ä»¶åï¼Œå¿…éœ€ï¼‰\n\n'
        "**ç¤ºä¾‹ï¼š**\n"
        '- {"source": "ç½—æµ®-é•¿å®å±±é•‡èåˆå‘å±•æˆ˜ç•¥.pptx"}\n'
        '- {"source": "plan.docx"}\n\n'
        "**æ³¨æ„ï¼š** æ–‡æ¡£å¯èƒ½å¾ˆé•¿ï¼ˆæ•°ä¸‡å­—ï¼‰ï¼Œä¼šæ¶ˆè€—å¤§é‡ Tokenã€‚è°¨æ…ä½¿ç”¨ã€‚"
    ),
)


# ==================== å·¥å…·åˆ—è¡¨ ====================

PLANNING_TOOLS = [
    document_list_tool,
    document_overview_tool,
    key_points_search_tool,
    knowledge_search_tool,
    chapter_content_tool,
    full_document_tool,
]

# å‘åå…¼å®¹ï¼šåˆ«å
planning_knowledge_tool = knowledge_search_tool
executive_summary_tool = document_overview_tool
chapter_summaries_list_tool = document_overview_tool
chapter_summary_tool = chapter_content_tool
chapter_context_tool = chapter_content_tool
context_around_tool = knowledge_search_tool


# ==================== æ—§ç‰ˆå·¥å…·ï¼ˆå…¼å®¹æ€§ï¼‰====================

def retrieve_planning_knowledge(
    query: str,
    top_k: int = DEFAULT_TOP_K,
    with_context: bool = True,
    context_chars: int = 300
) -> str:
    """æ£€ç´¢ä¹¡æ‘è§„åˆ’ç›¸å…³çŸ¥è¯†ï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰"""
    if not with_context or context_chars == 0:
        context_mode = "minimal"
    elif context_chars >= 500:
        context_mode = "expanded"
    else:
        context_mode = "standard"

    return search_knowledge(query, top_k, context_mode)


from langchain_core.tools import tool


@tool(response_format="content_and_artifact")
def retrieve_knowledge_detailed(query: str) -> tuple[str, list[Document]]:
    """æ£€ç´¢çŸ¥è¯†ï¼ˆAgentic RAG æ¨¡å¼ï¼Œå…¼å®¹æ—§ç‰ˆï¼‰"""
    db = get_vectorstore()
    retrieved_docs = db.similarity_search(query, k=DEFAULT_TOP_K)

    serialized = "\n\n".join(
        f"æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}\n"
        f"ä½ç½®: {doc.metadata.get('page', doc.metadata.get('paragraph', 'æœªçŸ¥'))}\n"
        f"å†…å®¹: {doc.page_content}"
        for doc in retrieved_docs
    )

    return serialized, retrieved_docs
