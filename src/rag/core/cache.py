"""
å‘é‡æ•°æ®åº“ç¼“å­˜ç®¡ç†å™¨
åŸºäº references/agent_skills çš„ Context Optimization æœ€ä½³å®è·µ

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. Embedding æ¨¡å‹ç¼“å­˜ï¼ˆè¿›ç¨‹çº§å•ä¾‹ï¼‰
2. å‘é‡æ•°æ®åº“è¿æ¥ç¼“å­˜
3. æŸ¥è¯¢ç»“æœç¼“å­˜ï¼ˆå¯é€‰ï¼Œä½¿ç”¨ LRU ç­–ç•¥ï¼‰
"""
import hashlib
import pickle
from pathlib import Path
from typing import Optional, List
from datetime import datetime, timedelta

import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.rag.config import (
    CHROMA_COLLECTION_NAME,
    CHROMA_PERSIST_DIR,
    EMBEDDING_MODEL_NAME,
)


class VectorStoreCache:
    """
    å‘é‡æ•°æ®åº“ç¼“å­˜ç®¡ç†å™¨

    åŠŸèƒ½ï¼š
    1. Embedding æ¨¡å‹ç¼“å­˜ï¼ˆè¿›ç¨‹çº§å•ä¾‹ï¼‰
    2. å‘é‡æ•°æ®åº“è¿æ¥ç¼“å­˜
    3. æŸ¥è¯¢ç»“æœç¼“å­˜ï¼ˆå¯é€‰ï¼‰

    ä½¿ç”¨åœºæ™¯ï¼š
    - å‡å°‘ Embedding æ¨¡å‹é‡å¤åŠ è½½
    - åŠ é€Ÿå‘é‡æ•°æ®åº“æŸ¥è¯¢
    - ç¼“å­˜å¸¸è§æŸ¥è¯¢ç»“æœ
    """

    def __init__(
        self,
        cache_dir: Optional[Path] = None,
        enable_query_cache: bool = True,
        cache_ttl: int = 3600  # ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 1 å°æ—¶
    ):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨

        Args:
            cache_dir: ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨ knowledge_base/cache
            enable_query_cache: æ˜¯å¦å¯ç”¨æŸ¥è¯¢ç»“æœç¼“å­˜
            cache_ttl: ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰
        """
        self.cache_dir = cache_dir or (CHROMA_PERSIST_DIR / "cache")
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        self.enable_query_cache = enable_query_cache
        self.cache_ttl = cache_ttl

        # ç¼“å­˜å®ä¾‹
        self._embedding_model = None
        self._vectorstore = None
        self._query_cache = {}  # å†…å­˜ç¼“å­˜ï¼š{query_hash: (result, timestamp)}
        self._cache_metadata = {}  # ç¼“å­˜å…ƒæ•°æ®ï¼š{query_hash: timestamp}

        print(f"âœ… ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼ˆç›®å½•: {self.cache_dir}ï¼‰")

    def get_embedding_model(self):
        """
        æ‡’åŠ è½½å¹¶ç¼“å­˜ Embedding æ¨¡å‹

        Returns:
            HuggingFaceEmbeddings å®ä¾‹
        """
        if self._embedding_model is None:
            from langchain_huggingface import HuggingFaceEmbeddings

            print("ğŸ“¥ æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹...")
            self._embedding_model = HuggingFaceEmbeddings(
                model_name=EMBEDDING_MODEL_NAME,
                encode_kwargs={"normalize_embeddings": True},
            )
            print(f"âœ… Embedding æ¨¡å‹å·²ç¼“å­˜: {EMBEDDING_MODEL_NAME}")

        return self._embedding_model

    def get_vectorstore(self):
        """
        æ‡’åŠ è½½å¹¶ç¼“å­˜å‘é‡æ•°æ®åº“

        Returns:
            Chroma å‘é‡æ•°æ®åº“å®ä¾‹
        """
        if self._vectorstore is None:
            from langchain_chroma import Chroma

            print("ğŸ“¥ æ­£åœ¨è¿æ¥å‘é‡æ•°æ®åº“...")
            self._vectorstore = Chroma(
                persist_directory=str(CHROMA_PERSIST_DIR),
                embedding_function=self.get_embedding_model(),
                collection_name=CHROMA_COLLECTION_NAME,
            )
            print(f"âœ… å‘é‡æ•°æ®åº“å·²ç¼“å­˜: {CHROMA_COLLECTION_NAME}")

        return self._vectorstore

    def cache_query_result(
        self,
        query: str,
        results: list,
        context_params: dict = None
    ) -> None:
        """
        ç¼“å­˜æŸ¥è¯¢ç»“æœ

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            results: æŸ¥è¯¢ç»“æœåˆ—è¡¨
            context_params: ä¸Šä¸‹æ–‡å‚æ•°ï¼ˆå¦‚ top_k, context_modeï¼‰
        """
        if not self.enable_query_cache:
            return

        # ç”Ÿæˆç¼“å­˜é”®ï¼ˆåŒ…å«æŸ¥è¯¢å’Œå‚æ•°ï¼‰
        cache_key = self._generate_cache_key(query, context_params)
        timestamp = datetime.now()

        # å†…å­˜ç¼“å­˜
        self._query_cache[cache_key] = (results, timestamp)

        # æŒä¹…åŒ–ç¼“å­˜ï¼ˆå¯é€‰ï¼‰
        cache_file = self.cache_dir / f"query_{cache_key}.pkl"
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump({
                    'results': results,
                    'timestamp': timestamp,
                    'query': query,
                    'params': context_params
                }, f)
        except Exception as e:
            print(f"âš ï¸  æŒä¹…åŒ–ç¼“å­˜å¤±è´¥: {e}")

    def get_cached_query(
        self,
        query: str,
        context_params: dict = None
    ) -> Optional[list]:
        """
        è·å–ç¼“å­˜çš„æŸ¥è¯¢ç»“æœ

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            context_params: ä¸Šä¸‹æ–‡å‚æ•°

        Returns:
            ç¼“å­˜çš„æŸ¥è¯¢ç»“æœï¼Œå¦‚æœä¸å­˜åœ¨æˆ–å·²è¿‡æœŸåˆ™è¿”å› None
        """
        if not self.enable_query_cache:
            return None

        cache_key = self._generate_cache_key(query, context_params)

        # å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
        if cache_key in self._query_cache:
            results, timestamp = self._query_cache[cache_key]

            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if datetime.now() - timestamp < timedelta(seconds=self.cache_ttl):
                print(f"ğŸ¯ å†…å­˜ç¼“å­˜å‘½ä¸­: {query[:50]}...")
                return results
            else:
                # è¿‡æœŸï¼Œåˆ é™¤ç¼“å­˜
                del self._query_cache[cache_key]

        # æ£€æŸ¥æŒä¹…åŒ–ç¼“å­˜
        cache_file = self.cache_dir / f"query_{cache_key}.pkl"
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    cached_data = pickle.load(f)

                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                cache_time = cached_data['timestamp']
                if datetime.now() - cache_time < timedelta(seconds=self.cache_ttl):
                    print(f"ğŸ¯ æŒä¹…åŒ–ç¼“å­˜å‘½ä¸­: {query[:50]}...")
                    # åŠ è½½åˆ°å†…å­˜ç¼“å­˜
                    self._query_cache[cache_key] = (
                        cached_data['results'],
                        cache_time
                    )
                    return cached_data['results']
                else:
                    # è¿‡æœŸï¼Œåˆ é™¤ç¼“å­˜æ–‡ä»¶
                    cache_file.unlink()
            except Exception as e:
                print(f"âš ï¸  è¯»å–æŒä¹…åŒ–ç¼“å­˜å¤±è´¥: {e}")

        return None

    def _generate_cache_key(self, query: str, params: dict = None) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            params: ä¸Šä¸‹æ–‡å‚æ•°

        Returns:
            MD5 å“ˆå¸Œå€¼ï¼ˆ8 ä½ï¼‰
        """
        # å°†å‚æ•°æ ‡å‡†åŒ–
        if params is None:
            params = {}

        # ç”Ÿæˆå“ˆå¸Œè¾“å…¥
        hash_input = f"{query}_{sorted(params.items())}"

        # è¿”å› MD5 å“ˆå¸Œçš„å‰ 8 ä½
        return hashlib.md5(hash_input.encode()).hexdigest()[:8]

    def clear_cache(self, older_than: int = None) -> int:
        """
        æ¸…ç†ç¼“å­˜

        Args:
            older_than: æ¸…ç†æ—©äº N ç§’çš„ç¼“å­˜ï¼ŒNone è¡¨ç¤ºæ¸…ç†å…¨éƒ¨

        Returns:
            æ¸…ç†çš„ç¼“å­˜æ•°é‡
        """
        count = 0
        current_time = datetime.now()

        # æ¸…ç†å†…å­˜ç¼“å­˜
        if older_than is None:
            count = len(self._query_cache)
            self._query_cache.clear()
        else:
            expired_keys = [
                key for key, (_, timestamp) in self._query_cache.items()
                if (current_time - timestamp).total_seconds() > older_than
            ]
            for key in expired_keys:
                del self._query_cache[key]
                count += 1

        # æ¸…ç†æŒä¹…åŒ–ç¼“å­˜
        if self.cache_dir.exists():
            for cache_file in self.cache_dir.glob("query_*.pkl"):
                try:
                    if older_than is None:
                        cache_file.unlink()
                        count += 1
                    else:
                        # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                        file_time = datetime.fromtimestamp(cache_file.stat().st_mtime)
                        if (current_time - file_time).total_seconds() > older_than:
                            cache_file.unlink()
                            count += 1
                except Exception as e:
                    print(f"âš ï¸  åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")

        print(f"ğŸ§¹ æ¸…ç†äº† {count} ä¸ªç¼“å­˜é¡¹")
        return count

    def get_cache_stats(self) -> dict:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯

        Returns:
            åŒ…å«ç¼“å­˜ç»Ÿè®¡çš„å­—å…¸
        """
        # ç»Ÿè®¡æŒä¹…åŒ–ç¼“å­˜æ–‡ä»¶
        persistent_cache_count = 0
        persistent_cache_size = 0
        if self.cache_dir.exists():
            for cache_file in self.cache_dir.glob("query_*.pkl"):
                persistent_cache_count += 1
                persistent_cache_size += cache_file.stat().st_size

        return {
            "memory_cache_count": len(self._query_cache),
            "persistent_cache_count": persistent_cache_count,
            "persistent_cache_size_mb": round(persistent_cache_size / 1024 / 1024, 2),
            "cache_dir": str(self.cache_dir),
            "query_cache_enabled": self.enable_query_cache,
            "cache_ttl_seconds": self.cache_ttl
        }


# å…¨å±€ç¼“å­˜å®ä¾‹
_vector_cache = None


def get_vector_cache() -> VectorStoreCache:
    """
    è·å–å…¨å±€å‘é‡ç¼“å­˜å®ä¾‹

    Returns:
        VectorStoreCache å•ä¾‹
    """
    global _vector_cache
    if _vector_cache is None:
        _vector_cache = VectorStoreCache()
    return _vector_cache


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("æµ‹è¯• VectorStoreCache")
    cache = VectorStoreCache()

    # æµ‹è¯•ç¼“å­˜ç»Ÿè®¡
    stats = cache.get_cache_stats()
    print(f"\nç¼“å­˜ç»Ÿè®¡ï¼š")
    for key, value in stats.items():
        print(f"  {key}: {value}")

    # æµ‹è¯•æ¸…ç†ç¼“å­˜ï¼ˆæ¸…ç†æ—©äº 1 å°æ—¶çš„ç¼“å­˜ï¼‰
    cache.clear_cache(older_than=3600)
