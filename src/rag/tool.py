"""
çŸ¥è¯†åº“æ£€ç´¢å·¥å…·ï¼ˆé€‚é… Planning Agentï¼‰
ä½¿ç”¨ Agentic RAG æ¨¡å¼ï¼Œè®© LLM è‡ªä¸»å†³å®šä½•æ—¶æ£€ç´¢
æ”¯æŒæ•´ä½“å±‚é¢çŸ¥è¯†è¯»å–ï¼Œæ›´é€‚åˆå¤æ‚å†³ç­–åœºæ™¯
"""
import os
from pathlib import Path
from typing import List

from langchain_core.documents import Document
from langchain_core.tools import Tool
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# å¯¼å…¥é…ç½®
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.rag.config import (
    CHROMA_COLLECTION_NAME,
    CHROMA_PERSIST_DIR,
    DEFAULT_TOP_K,
    EMBEDDING_MODEL_NAME,
)
from src.rag.context_manager import get_context_manager

# å…¨å±€å˜é‡ï¼ˆæ‡’åŠ è½½ï¼‰
_embedding_model = None
_vectorstore = None


def get_vectorstore():
    """
    æ‡’åŠ è½½å‘é‡æ•°æ®åº“
    é¿å…æ¯æ¬¡è°ƒç”¨éƒ½é‡æ–°åŠ è½½æ¨¡å‹
    """
    global _embedding_model, _vectorstore

    if _vectorstore is None:
        print("ğŸ“¥ æ­£åœ¨åŠ è½½çŸ¥è¯†åº“...")

        # åˆå§‹åŒ– Embedding æ¨¡å‹
        _embedding_model = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL_NAME,
            encode_kwargs={"normalize_embeddings": True},
        )

        # åŠ è½½å‘é‡æ•°æ®åº“
        if not CHROMA_PERSIST_DIR.exists():
            raise FileNotFoundError(
                f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {CHROMA_PERSIST_DIR}\n"
                f"è¯·å…ˆè¿è¡Œ: python src/rag/build.py"
            )

        _vectorstore = Chroma(
            persist_directory=str(CHROMA_PERSIST_DIR),
            embedding_function=_embedding_model,
            collection_name=CHROMA_COLLECTION_NAME,
        )

        print(f"âœ… çŸ¥è¯†åº“åŠ è½½å®Œæˆï¼ˆé›†åˆ: {CHROMA_COLLECTION_NAME}ï¼‰")

    return _vectorstore


def retrieve_planning_knowledge(
    query: str,
    top_k: int = DEFAULT_TOP_K,
    with_context: bool = True,
    context_chars: int = 300
) -> str:
    """
    æ£€ç´¢ä¹¡æ‘è§„åˆ’ç›¸å…³çŸ¥è¯†ï¼ˆé€‚é… Planning Agentï¼‰

    Args:
        query: æŸ¥è¯¢é—®é¢˜
        top_k: è¿”å›çš„åˆ‡ç‰‡æ•°é‡ï¼ˆPlanning Agent éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡ï¼‰
        with_context: æ˜¯å¦åŒ…å«å‘¨å›´ä¸Šä¸‹æ–‡ï¼ˆé˜¶æ®µ1æ–°å¢åŠŸèƒ½ï¼‰
        context_chars: ä¸Šä¸‹æ–‡å­—ç¬¦æ•°ï¼ˆä»…åœ¨ with_context=True æ—¶ç”Ÿæ•ˆï¼‰

    Returns:
        æ ¼å¼åŒ–çš„æ£€ç´¢ç»“æœï¼ŒåŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯
    """
    try:
        db = get_vectorstore()

        # ä½¿ç”¨æ›´é«˜çš„ k å€¼è·å–æ›´å¤šä¸Šä¸‹æ–‡ï¼ˆé€‚åˆ Planning Agentï¼‰
        results: List[Document] = db.similarity_search(query, k=top_k)

        if not results:
            return "âš ï¸  çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        # æ ¼å¼åŒ–ç»“æœï¼Œæä¾›æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_parts = []

        for idx, doc in enumerate(results, 1):
            # æå–å…ƒæ•°æ®
            source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
            page = doc.metadata.get("page", doc.metadata.get("paragraph", "æœªçŸ¥"))
            doc_type = doc.metadata.get("type", "æœªçŸ¥ç±»å‹")
            start_index = doc.metadata.get("start_index", 0)

            # æ„å»ºåŸºç¡€ä¸Šä¸‹æ–‡ç‰‡æ®µ
            context_part = [
                f"ã€çŸ¥è¯†ç‰‡æ®µ {idx}ã€‘",
                f"æ¥æº: {source}",
                f"ä½ç½®: ç¬¬{page}{doc_type}",
            ]

            # å¦‚æœå¯ç”¨äº†ä¸Šä¸‹æ–‡åŠŸèƒ½ï¼Œå°è¯•è·å–å‘¨å›´å†…å®¹
            if with_context and start_index > 0:
                try:
                    cm = get_context_manager()
                    ctx = cm.get_context_around_chunk(source, start_index, context_chars)

                    if "error" not in ctx:
                        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
                        if ctx['before']:
                            context_part.append(f"\nå‰æ–‡:\n{ctx['before'][:200]}...")

                        context_part.append(f"\næ ¸å¿ƒå†…å®¹:\n{doc.page_content}")

                        if ctx['after']:
                            context_part.append(f"\nåæ–‡:\n{ctx['after'][:200]}...")

                        context_part.append(f"\nğŸ’¡ æç¤º: ä½¿ç”¨ get_full_document('{source}') æŸ¥çœ‹å®Œæ•´æ–‡æ¡£")
                    else:
                        # å›é€€åˆ°åŸå§‹æ ¼å¼
                        context_part.append(f"\nå†…å®¹:\n{doc.page_content}")

                except Exception as e:
                    # ä¸Šä¸‹æ–‡è·å–å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ ¼å¼
                    context_part.append(f"\nå†…å®¹:\n{doc.page_content}")
            else:
                # ä¸ä½¿ç”¨ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨åŸå§‹æ ¼å¼
                context_part.append(f"\nå†…å®¹:\n{doc.page_content}")

            context_parts.append("\n".join(context_part))

        return "\n\n".join(context_parts)

    except Exception as e:
        return f"âŒ æŸ¥è¯¢çŸ¥è¯†åº“æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"


def retrieve_with_metadata(
    query: str,
    top_k: int = DEFAULT_TOP_K,
    source_filter: str | None = None,
):
    """
    å¸¦å…ƒæ•°æ®è¿‡æ»¤çš„æ£€ç´¢ï¼ˆé«˜çº§ç”¨æ³•ï¼‰

    Args:
        query: æŸ¥è¯¢é—®é¢˜
        top_k: è¿”å›çš„åˆ‡ç‰‡æ•°é‡
        source_filter: æŒ‰æ¥æºè¿‡æ»¤ï¼ˆä¾‹å¦‚ï¼š"luofu_strategy.pptx"ï¼‰

    Returns:
        æ–‡æ¡£åˆ—è¡¨ï¼ˆåŒ…å«å®Œæ•´å…ƒæ•°æ®ï¼‰
    """
    try:
        db = get_vectorstore()

        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        if source_filter:
            # ä½¿ç”¨å…ƒæ•°æ®è¿‡æ»¤
            results = db.similarity_search(
                query,
                k=top_k,
                filter={"source": source_filter},
            )
        else:
            results = db.similarity_search(query, k=top_k)

        return results

    except Exception as e:
        print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
        return []


# ==================== LangChain Tool å®šä¹‰ ====================
# è¿™ä¸ª Tool å¯ä»¥ç›´æ¥é›†æˆåˆ° Agent ä¸­

planning_knowledge_tool = Tool(
    name="search_rural_planning_knowledge",
    func=retrieve_planning_knowledge,
    description=(
        "ã€ä¹¡æ‘è§„åˆ’çŸ¥è¯†åº“ã€‘"
        "å½“ç”¨æˆ·è¯¢é—®å…³äºä¹¡æ‘è§„åˆ’ã€å†œä¸šå‘å±•ã€äº§ä¸šå¸ƒå±€ã€å†å²æ–‡åŒ–ã€"
        "æ—…æ¸¸å¼€å‘ã€æ”¿ç­–è§£è¯»ç­‰éœ€è¦å®è§‚å†³ç­–çš„é—®é¢˜æ—¶ï¼Œä½¿ç”¨æ­¤å·¥å…·ã€‚"
        "è¯¥å·¥å…·ä¼šè¿”å›ç›¸å…³çš„çŸ¥è¯†ç‰‡æ®µï¼Œå¸®åŠ©ä½ åšå‡ºæ›´å…¨é¢çš„è§„åˆ’å’Œå†³ç­–ã€‚"
        "\n\n"
        "ä½¿ç”¨åœºæ™¯ç¤ºä¾‹ï¼š"
        '- "åšç½—å¤åŸçš„å‘å±•å®šä½æ˜¯ä»€ä¹ˆï¼Ÿ"'
        '- "å¦‚ä½•è§„åˆ’ä¹¡æ‘æ—…æ¸¸äº§ä¸šï¼Ÿ"'
        '- "å½“åœ°æœ‰å“ªäº›å†å²æ–‡åŒ–èµ„æºï¼Ÿ"'
    ),
)

# ä¹Ÿå¯ä»¥ä½¿ç”¨ response_format="content_and_artifact" æ¨¡å¼ï¼ˆAgentic RAGï¼‰
# è®© LLM èƒ½å¤Ÿçœ‹åˆ°åŸå§‹æ–‡æ¡£å¯¹è±¡
from langchain_core.tools import tool

@tool(response_format="content_and_artifact")
def retrieve_knowledge_detailed(query: str) -> tuple[str, List[Document]]:
    """
    æ£€ç´¢çŸ¥è¯†ï¼ˆAgentic RAG æ¨¡å¼ï¼‰
    è¿”å›æ ¼å¼åŒ–æ–‡æœ¬ + åŸå§‹æ–‡æ¡£å¯¹è±¡
    """
    db = get_vectorstore()
    retrieved_docs = db.similarity_search(query, k=DEFAULT_TOP_K)

    # æ ¼å¼åŒ–å†…å®¹
    serialized = "\n\n".join(
        f"æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}\n"
        f"ä½ç½®: {doc.metadata.get('page', doc.metadata.get('paragraph', 'æœªçŸ¥'))}\n"
        f"å†…å®¹: {doc.page_content}"
        for doc in retrieved_docs
    )

    return serialized, retrieved_docs


# ==================== ä¸Šä¸‹æ–‡æŸ¥è¯¢å·¥å…·ï¼ˆé˜¶æ®µ1æ–°å¢ï¼‰====================

def get_full_document(source: str) -> str:
    """
    è·å–å®Œæ•´æ–‡æ¡£å†…å®¹ï¼ˆç”¨äºæ·±åº¦ç†è§£ï¼‰

    Args:
        source: æ–‡æ¡£æ¥æºï¼ˆæ–‡ä»¶åï¼‰

    Returns:
        å®Œæ•´æ–‡æ¡£å†…å®¹
    """
    try:
        cm = get_context_manager()
        result = cm.get_full_document(source)

        if "error" in result:
            return f"âŒ {result['error']}"

        return (
            f"ã€å®Œæ•´æ–‡æ¡£ã€‘\n"
            f"æ¥æº: {result['source']}\n"
            f"ç±»å‹: {result['doc_type']}\n"
            f"æ€»åˆ‡ç‰‡æ•°: {result['total_chunks']}\n"
            f"å†…å®¹é•¿åº¦: {len(result['content'])} å­—ç¬¦\n\n"
            f"å†…å®¹:\n{result['content']}"
        )

    except Exception as e:
        return f"âŒ è·å–æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"


def get_chapter_by_header(source: str, header_pattern: str) -> str:
    """
    æ ¹æ®æ ‡é¢˜è·å–ç« èŠ‚å†…å®¹

    Args:
        source: æ–‡æ¡£æ¥æºï¼ˆæ–‡ä»¶åï¼‰
        header_pattern: æ ‡é¢˜å…³é”®è¯ï¼ˆå¦‚"ç¬¬ä¸€ç« "ã€"äº§ä¸šå‘å±•"ç­‰ï¼‰

    Returns:
        ç« èŠ‚å†…å®¹
    """
    try:
        cm = get_context_manager()
        result = cm.get_chapter_by_header(source, header_pattern)

        if "error" in result:
            return f"âŒ {result['error']}"

        return (
            f"ã€ç« èŠ‚å†…å®¹ã€‘\n"
            f"æ¥æº: {result['source']}\n"
            f"ç« èŠ‚: {result['chapter_title']}\n"
            f"è¡ŒèŒƒå›´: {result['line_range']}\n\n"
            f"å†…å®¹:\n{result['content']}"
        )

    except Exception as e:
        return f"âŒ è·å–ç« èŠ‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"


def get_context_around(source: str, position: int, context_chars: int = 500) -> str:
    """
    è·å–æŒ‡å®šä½ç½®å‘¨å›´çš„ä¸Šä¸‹æ–‡

    Args:
        source: æ–‡æ¡£æ¥æºï¼ˆæ–‡ä»¶åï¼‰
        position: å­—ç¬¦ä½ç½®
        context_chars: å‰åä¸Šä¸‹æ–‡å­—ç¬¦æ•°

    Returns:
        åŒ…å«å‰æ–‡ã€å½“å‰ä½ç½®ã€åæ–‡çš„å­—ç¬¦ä¸²
    """
    try:
        cm = get_context_manager()
        result = cm.get_context_around_chunk(source, position, context_chars)

        if "error" in result:
            return f"âŒ {result['error']}"

        output = [
            f"ã€ä¸Šä¸‹æ–‡ç‰‡æ®µã€‘",
            f"æ¥æº: {result['source']}",
            f"èŒƒå›´: {result['context_range']}",
        ]

        if result['before']:
            output.append(f"\nå‰æ–‡:\n{result['before']}")

        output.append(f"\nå½“å‰ä½ç½®:\n{result['current']}")

        if result['after']:
            output.append(f"\nåæ–‡:\n{result['after']}")

        return "\n".join(output)

    except Exception as e:
        return f"âŒ è·å–ä¸Šä¸‹æ–‡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"


def list_available_documents(query: str = "") -> str:
    """
    åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ–‡æ¡£

    Returns:
        æ–‡æ¡£åˆ—è¡¨
    """
    try:
        cm = get_context_manager()
        cm._ensure_loaded()

        if not cm.doc_index:
            return "âš ï¸  çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡æ¡£"

        output = ["ã€å¯ç”¨æ–‡æ¡£åˆ—è¡¨ã€‘\n"]

        for idx, (source, doc_idx) in enumerate(cm.doc_index.items(), 1):
            output.append(
                f"{idx}. {source}\n"
                f"   ç±»å‹: {doc_idx.doc_type}\n"
                f"   åˆ‡ç‰‡æ•°: {len(doc_idx.chunks_info)}\n"
                f"   é¢„è§ˆ: {doc_idx.chunks_info[0]['content_preview'] if doc_idx.chunks_info else 'N/A'}\n"
            )

        return "\n".join(output)

    except Exception as e:
        return f"âŒ åˆ—å‡ºæ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"


# ==================== æ–°å¢çš„ LangChain Tools ====================

full_document_tool = Tool(
    name="get_full_document",
    func=get_full_document,
    description=(
        "ã€è·å–å®Œæ•´æ–‡æ¡£ã€‘"
        "å½“ä½ éœ€è¦é˜…è¯»æ•´ä¸ªæ–‡æ¡£æ¥ç†è§£å®Œæ•´çš„è§„åˆ’èƒŒæ™¯ã€æ”¿ç­–ç»†èŠ‚æˆ–æ–¹æ¡ˆå…¨è²Œæ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚"
        "è¿™æ¯”æ£€ç´¢ç‰‡æ®µæ›´é€‚åˆç†è§£å®è§‚ç»“æ„å’Œå®Œæ•´é€»è¾‘ã€‚\n\n"
        "å‚æ•°è¯´æ˜ï¼š"
        '- source: æ–‡æ¡£æ¥æºï¼ˆæ–‡ä»¶åï¼‰ï¼Œå¦‚ "plan.docx" æˆ– "strategy.pdf"'
        "\n\n"
        "ä½¿ç”¨åœºæ™¯ç¤ºä¾‹ï¼š"
        '- "æˆ‘è¦äº†è§£ç½—æµ®å±±å‘å±•æˆ˜ç•¥çš„å®Œæ•´å†…å®¹"'
        '- "æŸ¥çœ‹åšç½—å¤åŸè§„åˆ’çš„æ‰€æœ‰ç« èŠ‚"'
        "\n\n"
        "æç¤ºï¼šå¯ä»¥ä½¿ç”¨ list_available_documents å…ˆæŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ–‡æ¡£ã€‚"
    ),
)

chapter_context_tool = Tool(
    name="get_chapter_by_header",
    func=lambda params: get_chapter_by_header(**params),
    description=(
        "ã€è·å–ç« èŠ‚å†…å®¹ã€‘"
        "æ ¹æ®æ ‡é¢˜å…³é”®è¯è·å–ç‰¹å®šç« èŠ‚çš„å®Œæ•´å†…å®¹ã€‚é€‚åˆæŸ¥çœ‹æ–‡æ¡£ä¸­çš„æŸä¸ªä¸»é¢˜ç« èŠ‚ã€‚\n\n"
        "å‚æ•°è¯´æ˜ï¼ˆJSONæ ¼å¼ï¼‰ï¼š"
        '- source: æ–‡æ¡£æ¥æºï¼ˆæ–‡ä»¶åï¼‰'
        '- header_pattern: æ ‡é¢˜å…³é”®è¯ï¼ˆå¦‚"ç¬¬ä¸€ç« "ã€"äº§ä¸šå‘å±•"ã€"ç¯å¢ƒä¿æŠ¤"ç­‰ï¼‰'
        "\n\n"
        "ä½¿ç”¨åœºæ™¯ç¤ºä¾‹ï¼š"
        '- æŸ¥çœ‹ç¬¬ä¸€ç« : {"source": "plan.docx", "header_pattern": "ç¬¬ä¸€ç« "}'
        '- æŸ¥çœ‹äº§ä¸šè§„åˆ’: {"source": "strategy.pdf", "header_pattern": "äº§ä¸š"}'
        "\n\n"
        "æç¤ºï¼šæ”¯æŒæ ‡é¢˜çš„éƒ¨åˆ†åŒ¹é…ï¼Œä¸å¿…è¾“å…¥å®Œæ•´æ ‡é¢˜ã€‚"
    ),
)

document_list_tool = Tool(
    name="list_available_documents",
    func=list_available_documents,
    description=(
        "ã€åˆ—å‡ºå¯ç”¨æ–‡æ¡£ã€‘"
        "åˆ—å‡ºçŸ¥è¯†åº“ä¸­æ‰€æœ‰å¯ç”¨çš„æ–‡æ¡£åŠå…¶åŸºæœ¬ä¿¡æ¯ã€‚"
        "åœ¨ä½¿ç”¨å…¶ä»–æ–‡æ¡£å·¥å…·å‰ï¼Œå»ºè®®å…ˆä½¿ç”¨æ­¤å·¥å…·æŸ¥çœ‹æœ‰å“ªäº›æ–‡æ¡£å¯ç”¨ã€‚"
    ),
)

context_around_tool = Tool(
    name="get_context_around",
    func=lambda params: get_context_around(**params),
    description=(
        "ã€è·å–ä¸Šä¸‹æ–‡ã€‘"
        "è·å–æ–‡æ¡£ä¸­ç‰¹å®šä½ç½®å‘¨å›´çš„ä¸Šä¸‹æ–‡ï¼ˆå‰æ–‡+å½“å‰ä½ç½®+åæ–‡ï¼‰ã€‚"
        "ç”¨äºç†è§£æŸä¸ªè§‚ç‚¹æˆ–æ®µè½çš„å®Œæ•´è¯­å¢ƒã€‚\n\n"
        "å‚æ•°è¯´æ˜ï¼ˆJSONæ ¼å¼ï¼‰ï¼š"
        '- source: æ–‡æ¡£æ¥æºï¼ˆæ–‡ä»¶åï¼‰'
        '- position: å­—ç¬¦ä½ç½®ï¼ˆä»åˆ‡ç‰‡çš„ start_index å…ƒæ•°æ®è·å–ï¼‰'
        '- context_chars: ä¸Šä¸‹æ–‡å­—ç¬¦æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤500ï¼‰'
        "\n\n"
        "ä½¿ç”¨åœºæ™¯ï¼š"
        "éœ€è¦ç†è§£æŸä¸ªæ£€ç´¢ç»“æœçš„å‰åé€»è¾‘æ—¶ä½¿ç”¨ã€‚"
    ),
)


# ==================== é˜¶æ®µ2ï¼šæ‘˜è¦æŸ¥è¯¢å·¥å…· ====================

def get_executive_summary_tool_func(source: str) -> str:
    """
    è·å–æ–‡æ¡£çš„æ‰§è¡Œæ‘˜è¦ï¼ˆ200å­—ï¼‰

    Args:
        source: æ–‡æ¡£æ¥æºï¼ˆæ–‡ä»¶åï¼‰

    Returns:
        æ‰§è¡Œæ‘˜è¦
    """
    try:
        cm = get_context_manager()
        result = cm.get_executive_summary(source)

        if "error" in result:
            return f"âŒ {result['error']}"

        if not result.get("executive_summary"):
            return f"âš ï¸  {result.get('message', 'è¯¥æ–‡æ¡£æš‚æ— æ‰§è¡Œæ‘˜è¦')}"

        return (
            f"ã€æ‰§è¡Œæ‘˜è¦ã€‘\n"
            f"æ¥æº: {result['source']}\n"
            f"ç±»å‹: {result['doc_type']}\n\n"
            f"{result['executive_summary']}"
        )

    except Exception as e:
        return f"âŒ è·å–æ‰§è¡Œæ‘˜è¦æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"


def list_chapter_summaries_tool_func(source: str) -> str:
    """
    åˆ—å‡ºæ–‡æ¡£çš„æ‰€æœ‰ç« èŠ‚æ‘˜è¦

    Args:
        source: æ–‡æ¡£æ¥æºï¼ˆæ–‡ä»¶åï¼‰

    Returns:
        ç« èŠ‚æ‘˜è¦åˆ—è¡¨
    """
    try:
        cm = get_context_manager()
        result = cm.list_chapter_summaries(source)

        if "error" in result:
            return f"âŒ {result['error']}"

        if not result.get("chapters"):
            return f"âš ï¸  {result.get('message', 'è¯¥æ–‡æ¡£æš‚æ— ç« èŠ‚æ‘˜è¦')}"

        output = [
            f"ã€ç« èŠ‚æ‘˜è¦åˆ—è¡¨ã€‘",
            f"æ¥æº: {result['source']}",
            f"æ€»ç« èŠ‚æ•°: {result['total_chapters']}\n"
        ]

        for idx, chapter in enumerate(result['chapters'], 1):
            output.append(
                f"\n{idx}. {chapter['title']}\n"
                f"   æ‘˜è¦: {chapter['summary']}\n"
                f"   è¦ç‚¹: {'; '.join(chapter.get('key_points', [])[:3])}"
            )

        return "\n".join(output)

    except Exception as e:
        return f"âŒ è·å–ç« èŠ‚æ‘˜è¦æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"


def get_chapter_summary_tool_func(params: dict) -> str:
    """
    è·å–ç‰¹å®šç« èŠ‚çš„æ‘˜è¦

    Args:
        params: åŒ…å« source å’Œ chapter_pattern çš„å­—å…¸

    Returns:
        ç« èŠ‚æ‘˜è¦
    """
    try:
        source = params.get("source")
        chapter_pattern = params.get("chapter_pattern")

        if not source or not chapter_pattern:
            return "âŒ ç¼ºå°‘å¿…è¦å‚æ•°ï¼šsource å’Œ chapter_pattern"

        cm = get_context_manager()
        result = cm.get_chapter_summary(source, chapter_pattern)

        if "error" in result:
            return f"âŒ {result['error']}"

        output = [
            f"ã€ç« èŠ‚æ‘˜è¦ã€‘",
            f"æ¥æº: {result['source']}",
            f"ç« èŠ‚: {result['chapter_title']}",
            f"çº§åˆ«: {result['level']}",
            f"ä½ç½®: {result['position']}\n",
            f"æ‘˜è¦:\n{result['summary']}\n",
            f"å…³é”®è¦ç‚¹:"
        ]

        for point in result.get('key_points', []):
            output.append(f"  â€¢ {point}")

        return "\n".join(output)

    except Exception as e:
        return f"âŒ è·å–ç« èŠ‚æ‘˜è¦æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"


def search_key_points_tool_func(params: dict) -> str:
    """
    åœ¨å…³é”®è¦ç‚¹ä¸­æœç´¢å…³é”®è¯

    Args:
        params: åŒ…å« query å’Œå¯é€‰ sources çš„å­—å…¸

    Returns:
        åŒ¹é…çš„è¦ç‚¹åˆ—è¡¨
    """
    try:
        query = params.get("query")
        sources = params.get("sources")  # å¯é€‰

        if not query:
            return "âŒ ç¼ºå°‘å¿…è¦å‚æ•°ï¼šquery"

        cm = get_context_manager()

        # å¤„ç† sources å‚æ•°
        sources_list = None
        if sources:
            if isinstance(sources, str):
                sources_list = [sources]
            elif isinstance(sources, list):
                sources_list = sources

        result = cm.search_key_points(query, sources_list)

        if result['total_matches'] == 0:
            return f"âš ï¸  æœªæ‰¾åˆ°åŒ…å« '{query}' çš„è¦ç‚¹"

        output = [
            f"ã€å…³é”®è¦ç‚¹æœç´¢ã€‘",
            f"æŸ¥è¯¢: {result['query']}",
            f"åŒ¹é…æ•°é‡: {result['total_matches']}\n"
        ]

        for match in result['matches']:
            output.append(
                f"æ¥æº: {match['source']}\n"
                f"è¦ç‚¹: {match['point']}\n"
            )

        return "\n".join(output)

    except Exception as e:
        return f"âŒ æœç´¢è¦ç‚¹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"


# ==================== æ–°å¢çš„ LangChain Toolsï¼ˆé˜¶æ®µ2ï¼‰====================

executive_summary_tool = Tool(
    name="get_executive_summary",
    func=get_executive_summary_tool_func,
    description=(
        "ã€è·å–æ‰§è¡Œæ‘˜è¦ã€‘"
        "å¿«é€Ÿäº†è§£æ–‡æ¡£çš„æ ¸å¿ƒå†…å®¹ï¼ˆ200å­—å·¦å³æ‘˜è¦ï¼‰ã€‚"
        "é€‚åˆåœ¨æ—¶é—´æœ‰é™æˆ–éœ€è¦å¿«é€Ÿæµè§ˆæ–‡æ¡£æ—¶ä½¿ç”¨ã€‚\n\n"
        "å‚æ•°è¯´æ˜ï¼š"
        '- source: æ–‡æ¡£æ¥æºï¼ˆæ–‡ä»¶åï¼‰'
        "\n\n"
        "ä½¿ç”¨åœºæ™¯ç¤ºä¾‹ï¼š"
        '- "è¿™ä¸ªè§„åˆ’æ–‡æ¡£çš„æ ¸å¿ƒç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ"'
        '- "å¿«é€Ÿäº†è§£è¿™ä¸ªæ”¿ç­–çš„ä¸»è¦å†…å®¹"'
        "\n\n"
        "æç¤ºï¼šæ‰§è¡Œæ‘˜è¦åŒ…å«ç›®æ ‡ã€å®šä½ã€å…³é”®æŒ‡æ ‡å’Œé‡ç‚¹æªæ–½ã€‚"
    ),
)

chapter_summaries_list_tool = Tool(
    name="list_chapter_summaries",
    func=list_chapter_summaries_tool_func,
    description=(
        "ã€åˆ—å‡ºç« èŠ‚æ‘˜è¦ã€‘"
        "åˆ—å‡ºæ–‡æ¡£çš„æ‰€æœ‰ç« èŠ‚æ‘˜è¦ï¼Œæµè§ˆæ–‡æ¡£ç»“æ„ã€‚"
        "æ¯ä¸ªç« èŠ‚åŒ…å«æ‘˜è¦å’Œå…³é”®è¦ç‚¹ã€‚\n\n"
        "å‚æ•°è¯´æ˜ï¼š"
        '- source: æ–‡æ¡£æ¥æºï¼ˆæ–‡ä»¶åï¼‰'
        "\n\n"
        "ä½¿ç”¨åœºæ™¯ç¤ºä¾‹ï¼š"
        '- "è¿™ä¸ªè§„åˆ’æ–‡æ¡£æœ‰å“ªäº›ç« èŠ‚ï¼Ÿ"'
        '- "æµè§ˆæ–‡æ¡£çš„ç»“æ„å’Œä¸»è¦å†…å®¹"'
        "\n\n"
        "æç¤ºï¼šä¸ get_chapter_by_header ä¸åŒï¼Œæ­¤å·¥å…·åªè¿”å›æ‘˜è¦ï¼Œä¸è¿”å›å®Œæ•´å†…å®¹ã€‚"
    ),
)

chapter_summary_tool = Tool(
    name="get_chapter_summary",
    func=get_chapter_summary_tool_func,
    description=(
        "ã€è·å–ç« èŠ‚æ‘˜è¦ã€‘"
        "è·å–ç‰¹å®šç« èŠ‚çš„æ‘˜è¦å’Œå…³é”®è¦ç‚¹ï¼ˆä¸è¿”å›å®Œæ•´å†…å®¹ï¼‰ã€‚"
        "æ¯” get_chapter_by_header æ›´ç®€æ´ï¼Œåªè¿”å›æ‘˜è¦ç‰ˆæœ¬ã€‚\n\n"
        "å‚æ•°è¯´æ˜ï¼ˆJSONæ ¼å¼ï¼‰ï¼š"
        '- source: æ–‡æ¡£æ¥æºï¼ˆæ–‡ä»¶åï¼‰'
        '- chapter_pattern: ç« èŠ‚æ ‡é¢˜å…³é”®è¯ï¼ˆå¦‚"ç¬¬ä¸€ç« "ã€"äº§ä¸šå‘å±•"ç­‰ï¼‰'
        "\n\n"
        "ä½¿ç”¨åœºæ™¯ç¤ºä¾‹ï¼š"
        '- è·å–ç¬¬ä¸€ç« æ‘˜è¦: {"source": "plan.docx", "chapter_pattern": "ç¬¬ä¸€ç« "}'
        '- è·å–äº§ä¸šç« èŠ‚æ‘˜è¦: {"source": "strategy.pdf", "chapter_pattern": "äº§ä¸š"}'
        "\n\n"
        "æç¤ºï¼šæ”¯æŒæ ‡é¢˜çš„éƒ¨åˆ†åŒ¹é…ï¼Œè¿”å›æ‘˜è¦+è¦ç‚¹ï¼Œä¸è¿”å›å®Œæ•´å†…å®¹ã€‚"
    ),
)

key_points_search_tool = Tool(
    name="search_key_points",
    func=search_key_points_tool_func,
    description=(
        "ã€æœç´¢å…³é”®è¦ç‚¹ã€‘"
        "åœ¨æ‰€æœ‰æ–‡æ¡£çš„å…³é”®è¦ç‚¹ä¸­æœç´¢å…³é”®è¯ã€‚"
        "è¦ç‚¹æ˜¯ä»æ–‡æ¡£ä¸­æå–çš„10-15æ¡æ ¸å¿ƒä¿¡æ¯ã€‚\n\n"
        "å‚æ•°è¯´æ˜ï¼ˆJSONæ ¼å¼ï¼‰ï¼š"
        '- query: æœç´¢å…³é”®è¯ï¼ˆå¿…éœ€ï¼‰'
        '- sources: é™åˆ¶æœç´¢çš„æ–‡æ¡£åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰'
        "\n\n"
        "ä½¿ç”¨åœºæ™¯ç¤ºä¾‹ï¼š"
        '- æœç´¢æ—…æ¸¸ç›¸å…³: {"query": "æ—…æ¸¸"}'
        '- æœç´¢ç‰¹å®šæ–‡æ¡£: {"query": "ç›®æ ‡", "sources": "plan.docx"}'
        '- æœç´¢å¤šä¸ªæ–‡æ¡£: {"query": "æŠ•èµ„", "sources": ["plan1.docx", "plan2.docx"]}'
        "\n\n"
        "æç¤ºï¼šå…³é”®è¦ç‚¹æ˜¯é¢„å…ˆæå–çš„ï¼Œæ¯”å…¨æ–‡æ£€ç´¢æ›´ç²¾ç¡®ã€æ›´å¿«é€Ÿã€‚"
    ),
)
